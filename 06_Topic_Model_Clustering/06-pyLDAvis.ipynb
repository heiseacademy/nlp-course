{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierung mit `pyLDAvis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Teil hast du LDA als Topic Model ausprobiert. Die Ergebnisse waren nicht so gut interpretierbar, daher interessierst du dich für deren Struktur, um evtl. etwas verbessern zu können.\n",
    "\n",
    "Dabei unterstützt dich `pyLDAvis` als Visualisierungstool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACHTUNG Colab-User\n",
    "\n",
    "Du musst nach der Installation von pyLDAvis die Runtime-Umgebung neu starten, sonst funktioniert das nicht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend führst du die Vektorisierung durch. Das kann auch wieder einen Augenblick dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words, min_df=5)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Nutzung von `min_df=5` ist die Matrix einigermaßen übersichtlich geblieben!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model mit LDA berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Aufrufsyntax des Topic Models ist sehr ähnlich zu der des `TfidfVectorizers`, nur dass du hier keine Transformation durchführen musst, daher heißt die Methode nur `fit`. Der Aufruf kann ein paar Sekunden dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = num_topics, random_state=42)\n",
    "lda.fit(count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Topics übersichtlich darstellen zu können, kannst du eine Funktion nutzen, die einen separaten Dataframe dafür aufbaut. Dazu iterierst du über alle Topics (`n_components` im Topic Model) und ermittelst jeweils immer die wichtigsten Wörter. `argsort` sortiert aufsteigend, daher sind es die letzten Indizes im Array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_table(model, feature_names, n_top_words = 20):\n",
    "    word_dict = {}\n",
    "    \n",
    "    for i in range(model.n_components):\n",
    "        # ermittle für jedes Topic die größten Werte\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words-1:-1]\n",
    "        words = [feature_names[key] for key in words_ids]\n",
    "        # und füge die entsprechenden Worte im Klartext dem Dictionary hinzu\n",
    "        word_dict['Topic #%02d' % i] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table(lda, count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis sieht schon ziemlich gut aus. Abgesehen von dem ersten Topic (#00) sind alle ziemlich gut interpretierbar!\n",
    "\n",
    "Wie viel Raum nehmen die einzelnen Topics in Anspruch? Das kannst du ausrechnen, indem du die Transformation durch das Topic Model vornehmen lässt und dadurch die Gewichte ermittelst. Die Summierung geht über die Achse, in der alle Dokumente mit ihren jeweiligen Anteilen am Topic enthalten haben. Um Prozentzahlen zu erhalten, musst du noch mit 100 multipltizieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = lda.transform(count_vectors)\n",
    "W.sum(axis=0)/W.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider nummeriert pyLDAvis die Topics beginnend mit 1. Du kannst in der Darstellung erkennen, dass die Topics 5 und 8 die größten sind, das passt zu den Zahlen oben. Die Überlappungen sind nicht so einfach zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "lda_display = pyLDAvis.sklearn.prepare(lda, count_vectors, count_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyLDAvis` verschafft dir einen guten Überblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `pyLDAvis` kannt du dir einen guten Überblick über die Struktur eines LDA-Modells verschaffen. Leider geht das auch nur für LDA-Modelle und nicht für NMF oder SVD. Spätestens hier hast du also ein Argument, das für LDA spricht."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
