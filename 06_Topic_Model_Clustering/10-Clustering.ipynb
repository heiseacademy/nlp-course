{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den letzten Teilen hast du dich ausführlich mit Topic Models beschäftigt. Bei Textdaten ist das eine äußerst sinnvolle Möglichkeit, wie du Strukturen entdecken kannst.\n",
    "\n",
    "Bei strukturierten Daten hingegen wird häufig mit Clustering gearbeitet. Auch Textdaten kannst du clustern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend führst du die Vektorisierung durch. Das kann auch wieder einen Augenblick dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=5)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Nutzung von `min_df=5` ist die Matrix einigermaßen übersichtlich geblieben!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering mit K-Means durchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Clusterin-Algorithmen bei `scikit-learn` nutzen das identische API wie die Topic Models. Das ist natürlich äußerst praktisch, weil du so gut wie nichts ändern musst.\n",
    "\n",
    "Für größere Datenmengen kannst du `MiniBatchKMeans` verwenden, bei deutlich weniger als 10.000 Dokumenten solltest du besser `KMeans` selbst verwenden. Die `import`-Anweisung ist so gebaut, dass dur nur dort eine Ersetzung vornehmen musst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans as KMeans\n",
    "k_means = KMeans(n_clusters=10, random_state=42)\n",
    "k_means.fit(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ergebnis steht dir nun eine `labels_`-Variable zur Verfügung, die dir für jedes Dokument sagt, zu welchem Cluster es gehört:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `numpy` kannst du die Multiplizäten ermitteln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(k_means.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Cluster `2` ist deutlich größer als die anderen. Das ist ein Effekt, den du häufiger bei Clustering beobachten kannst: ein Cluster fungiert als *Sammler*, der einfach alles aufnimmt. In diesem Fall ist es gar nicht so ausgeprägt, weli ein anderer Cluster (`0`) auch ziemlich groß ist.\n",
    "\n",
    "Wenn du die TF/IDF-Gewichte der Cluster aufsummierst, findest du die dominierenden Wörter und kannst diese in Wordclouds einzeichnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "def wordcloud_clusters(model, vectors, features, no_top_words=40):\n",
    "    for cluster in np.unique(model.labels_):\n",
    "        size = {}\n",
    "        words = vectors[model.labels_ == cluster].sum(axis=0).A[0]\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall ist das Ergebnis richtig gut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_clusters(k_means, tfidf_vectors, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering ist gut geeignet für kurze Texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da beim Clustering jedem Dokument genau ein Cluster zugeordnet wird, ist es etwas rigoroser als die Topic Models. Häufig ist es auch für Menschen schwer, einem Dokument nur ein einziges Thema zuzuordnen.\n",
    "\n",
    "Bei kurzen Dokumenten oder solchen, die klar definierte Themen haben, geht das hingegen ziemlich gut. Daher kannst du auch für den Heise-Newsticker damit gute Ergebnisse erzielen, die denen des Topic Models sehr nahe kommen. Es gibt auch noch ein paar Vorteile: so kannst du *hierarchisch clustern*, weil du für jedes Dokument den Cluster kennst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
