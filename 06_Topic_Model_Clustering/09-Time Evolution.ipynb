{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeitevolution von Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den letzten Kapiteln hast du viel zu unterschiedlichen Topic Models gelernt. Allerdings waren diese alle statisch. Dabei sind die Newsticker-Meldungen *zeitabhängig*.\n",
    "\n",
    "Es stellt sich als schwierig heraus, *zeitabhängige Topic Models* zu berechnen, weil sich die einzelnen Topics nicht mehr zuordnen lassen. Stattdessen ist es geschickter, wenn du ein einziges Topic Model berechnest und überprüfst, wie sich die Anteile an den Topics im Laufe der Zeit verschieben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend führst du die Vektorisierung durch. Das kann auch wieder einen Augenblick dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=5)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Nutzung von `min_df=5` ist die Matrix einigermaßen übersichtlich geblieben!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model mit NMF berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Aufrufsyntax des Topic Models ist sehr ähnlich zu der des `TfidfVectorizers`, nur dass du hier keine Transformation durchführen musst, daher heißt die Methode nur `fit`. Der Aufruf kann ein paar Sekunden dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "nmf = NMF(n_components = num_topics)\n",
    "nmf.fit(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Topics übersichtlich darstellen zu können, kannst du eine Funktion nutzen, die einen separaten Dataframe dafür aufbaut. Dazu iterierst du über alle Topics (`n_components` im Topic Model) und ermittelst jeweils immer die wichtigsten Wörter. `argsort` sortiert aufsteigend, daher sind es die letzten Indizes im Array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_table(model, feature_names, n_top_words = 20):\n",
    "    word_dict = {}\n",
    "    \n",
    "    for i in range(model.n_components):\n",
    "        # ermittle für jedes Topic die größten Werte\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words-1:-1]\n",
    "        words = [feature_names[key] for key in words_ids]\n",
    "        # und füge die entsprechenden Worte im Klartext dem Dictionary hinzu\n",
    "        word_dict['Topic #%02d' % i] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schau dir jetzt die Tabelle mit den Themen an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table(nmf, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun setzt du ein Feld im `DataFrame` auf den Monat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = pd.to_datetime(df[\"datePublished\"], utc=True).dt.strftime(\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend iterierst du über alle Monate und wendest das Topic Model für die Daten des entsprechenden Monats an. Die Anteil der Topics speicherst du in `month_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "month_data = []\n",
    "for month in tqdm(np.unique(np.unique(df[\"month\"]))):\n",
    "    W_month = nmf.transform(tfidf_vectors[np.array(df[\"month\"] == month)])\n",
    "    month_data.append([month] + list(W_month.sum(axis=0)/W_month.sum()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit die Topics nicht nur Nummern haben, merkst du dir die beiden wichtigsten Wörter für jedes Topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = tfidf_vectorizer.get_feature_names()\n",
    "for topic in nmf.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    topic_names.append(\"Topic \" + top_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das fügst du nun alles in einem `DataFrame` zusammen und plottest das Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.DataFrame(month_data, columns=[\"month\"] + topic_names).set_index(\"month\")\n",
    "df_month.plot.area(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist nicht nur optisch schön, sonst auch gut interpretierbar. Am Anfang des Jahres scheint Apple und iPhone keine große Rolle gespielt zu haben, das hat sich später deutlich verändert zu Lasten des generischen Topics. Corona und die Warnapp haben auch ab März zugenommen, was auch der eigenen Wahrnehmung entspricht.\n",
    "\n",
    "Neben diesen Erkenntnissen ist es besonders spannend, dass das alles *unüberwacht* und damit auch *unvoreingenommen* entstanden ist. Dieselbe Methode kannst du auch für andere (zeitabhängige) Daten einsetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeitevolution ist spannend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die hier erklärten Methoden kannst du nicht nur für Topics, sondern auch für Cluster einsetzen. Dann bist du schon fast im Bereich der *Klassifikation*, mit dem wir uns im nächsten Kapitel beschäftigen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
