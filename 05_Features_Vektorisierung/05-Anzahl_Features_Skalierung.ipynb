{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anzahl Features und Skalierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig bzw. *fast immer* werden Wörter oder Kombinationen von Wörtern als Features für die Vektorisierung verwendet. Allerdings wirst du merken, dass das leicht zu sehr großen Vektordimensionen führen kann.\n",
    "\n",
    "In diesem Teil wirst du Methoden kennenlernen, wie du die Dimension des Vektorraums kontrollieren kannst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Skalierungseffekt zu sehen, arbeitest du mit der gesamten Datenmenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles\", sql, index_col=\"id\", parse_dates=[\"datePublished\"])\n",
    "df[\"full_text\"] = df[\"title\"] + \"\\n\" + df[\"header\"] + df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiver Ansatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt versuchst du nun, die Dokumente zu vektorisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"full_text\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, sind das viel zu viele Features geworden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl der Wortarten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prinzipiell könntest du nun die `lemmas` verwenden, was die Anzahl der Features schon beträchtlich reduziert. Du gehst aber gleich einen Schritt weiter und nutzt die Kombination aus Substantiven, Adjektiven und Verben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist immer noch sehr groß und sollte weiter reduziert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoppwörter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoppwörter sind Wörter, die keinen (oder wenig) Inhalt tragen. Du kennst sie noch von den Wordclouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der gespeicherten Elemente ist nun ganz schön gesunken, die Dimensionen hingegen haben sich nicht stark verändert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `min_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `min_df` kannst du angeben, in wievielen Dokumenten Wörter mindestens vorkommen müssen, um berücksichtigt zu werden. Wenn du eine Ganzzahl angibt, ist damit die Anzahl gemeint, bei einer Kommazahl der Prozenzsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, min_df=5)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das hat erhebliche Auswirkungen und die Dimension des Vektorraums erheblich reduziert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `max_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt auch noch den ergänzenden Parameter `max_df`, der Wörter ausschließen kann, die in *zu vielen Dokumenten* vorkommen. Diese haben dann ohnehin eine geringe Auszeichnungsqualität und du kannst sie weglassen. Hier ist eine Angabe als Prozentanteil fast immer sinnvoller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Parameter hat insgesamt nur einen geringen Einfluss auf die Dimension des Vektorraums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzahl der Features begrenzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst auch einfach die Anzahl der Features begrenzen. `scikit-learn` nutzt dann einfach die häufigsten Wörter und ignoriert alle anderen. Manchmal kann das sinnvoll sein, aber zu einem Stück verlierst du auch die Kontrolle über die Auswahl der Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, max_features=10000)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achtung bei Bigrammen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits angesprochen können Bigramme den Feature-Space extrem aufblähen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=5)\n",
    "count_vectors = count_vectorizer.fit_transform(df[\"nav\"])\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-Engineering ist wichtig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Feature-Eningeering und die Optimierung kannst du fast nicht überschätzen! Hier hast du einen tollen Hebel zur Verfügung, der dir "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
