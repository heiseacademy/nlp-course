{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vektorisierung mit Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Teil hast du gelernt, dass Dokumente für Machine Learning in Vektoren umgewandelt werden (müssen). Dafür gibt es unterschiedliche Verfahren. In diesem Teil arbeitest du mit dem [*Bag-of-Words*-Modell](https://en.wikipedia.org/wiki/Bag-of-words_model) für die Vektorisierung.\n",
    "\n",
    "Das Bag-of-Words-Modell kannst du dir wirklich wie einen Sack voller Wörter vorstellen, der richtig durchgeschüttelt wird. Die Reihenfolge der Wörter geht dabei komplett verloren und auch der Zusammenhang der einzelnen Wörter. Wenn Wörter mehrfach in einem Dokument vorkommen, sind sie auch häufiger im Sack vorhanden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spielzeugdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden wir mit Spielzeugdaten arbeiten, damit die Vektoren nicht zu groß werden und du die Algorithmen noch im Detail nachvollziehen kannst. Später wirst du dann wieder auf die echten Daten aus den Newsticker wechseln.\n",
    "\n",
    "Betrachte zunächst diese beiden Dokumente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Max schaut sich gerne Filme an. Laura mag auch gerne Filme.\"\n",
    "s2 = \"Stefanie schaut sich gerne Fußballspiele an.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du könntest diese Dokumente jetzt durch `spacy` tokenisieren und analysieren lassen. Das wirst du später auch noch duchführen bzw. die linguistisch analsierten Daten verwenden. Versuche es zunächst einfacher und nutze den `CountVectorizer` von `scikit-learn`, um die Dokumente direkt zu vektorisieren:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektorisierung mit dem `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst initialisierst du den `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend rufst du die Funktion `fit_transform` auf, die gleich zwei Dinge für dich erledigt:\n",
    "* Sie bestimmt das Vokabular der Dokumente, die du ihr übergibst.\n",
    "* Sie übersetzt die Dokumente in *Bag-of-Words*-Vektoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors = count_vectorizer.fit_transform([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `count_vectors` sind nun eine dünn besetzte Matrix (die sog. *Document-Term-Matrix*):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektoren und die *Document-Term-Matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die kannst du in eine echte Matrix wandeln und ausgebene lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Vokabular stellt dir der `CountVectorizer` mit der Funktion `get_feature_names()` zur Verfügung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Document-Term-Matrix lässt sich schöner mit `pandas` darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(count_vectors.todense(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du sehen kannst, sind die Sätze ganz schön *zerhackt* worden. Die Reihenfolge und der Kontext der Worte ist komplett verloren gegangen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statt der Wörter alleine kannst du als Features auch *Bigramme* verwenden, d.h. Zwei-Wort-Kombinationen. Das geht fast genauso einfach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer2 = CountVectorizer(ngram_range=(2,2))\n",
    "count_vectors2 = count_vectorizer2.fit_transform([s1, s2])\n",
    "count_vectors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Matrix ist nicht viel größer geworden, was damit zusammenhängt, dass fast jedes Wort auch bisher nur einaml vorgekommen ist. Gleiches gilt natürlich für die Kombinationen. In größeren Dokumentenmengen führt die Verwendung von Bigrammen normalerweise zu einer Explosion des Vokabulars.\n",
    "\n",
    "Schau dir die Dokumenten-Term-Matrix an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_vectors2.todense(), columns=count_vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Dokumenten-Term-Matrix stellst du jetzt transponiert dar, weil sie dann einfacher zu lesen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_vectors2.todense(), columns=count_vectorizer2.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfache Vektorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words ist die erste Vektorisierung, die du kennengelernt hast. Sie ist sehr einfach, erfüllt aber für viele Textanalyse-Aufgaben alle Anforderungen und produziert im Allgemeinen schon gute Ergebnisse.\n",
    "\n",
    "Nach und nach werden wir das verfeinern, indem wir die Features optimieren und die Zahlen in der Matrix skalieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
