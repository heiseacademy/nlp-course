{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words vs. TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Bag-of-Words-Modell sind alle Wörter eines Dokuments *gleichberechtigt*. Dabei weißt du, dass es Wörter gibt, die eine unterschiedliche hohe *Auszeichungsqualität* haben.\n",
    "\n",
    "Einige Wörter (wie Artikel usw.) haben von sich aus einen geringen Aussagewert. Die Auszeichnungsqualität anderer Wörter ist jedoch hochgradig domänenespezifisch. Ein i.A. sehr unübliches Wort kann in einem speziellen Korpus sehr häufig vorkommen. Dazu kannst du diesen Kurs hier betrachten: im letzten Kapitel wurde sehr häufig das Wort *linguistisch* verwendet, was im allgemeinen Sprachgebrauch eher selten sein dürfte.\n",
    "\n",
    "Wir wollen also ein Maß finden, mit dem wir die Aussagekraft bewerten können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spielzeugdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du beginnst wieder mit den ganz einfach Beispiel-Dokumente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Max schaut sich gerne Filme an. Laura mag auch gerne Filme.\"\n",
    "s2 = \"Stefanie schaut sich gerne Fußballspiele an.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektorisierung mit dem `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und diese vektorisierst du wie gewohnt mit dem `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectors = count_vectorizer.fit_transform([s1, s2])\n",
    "count_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termfrequenz und invertierte Dokumentenfrequenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Termfrequenz hast du schon kennengelernt, das ist die Häufigkeit, in der ein einzelnes Wort auftaucht.\n",
    "\n",
    "Für die invertierte Dokumentenfrequenz betrachtest du, wie oft ein Wort im allen Dokumenten zusammen vorkommt. Je häufiger ein Wort ist, desto unspezifischer ist es. Deswegen wird die invertierte Dokumentenfrequenz für das Wort *i* definiert als: \n",
    "\n",
    "$\\mathrm{IDF}_i = \\frac{n}{1 + log F_i}$\n",
    "\n",
    "Zum Glück musst du dich wenig mit solchen Formeln rumplagen, weil `scikit-learn` das alles für dich berechnen kann. Im nächsten Abschnitt siehst du, wie das geht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation in TF/IDF-Raum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die TF/IDF-Vektoren können aus den `count_vectors` mithilfe des `TfidfTransfomers` berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend rufst du die Funktion `fit_transform` auf, die mithilfe der `count_vectors` die Häufigkeiten der Wörter bestimmt und die einzelnen Gewicht so verändert, das häufigere Wörter bestraft werden und niedrigere Gewichte erhalten. Das Resultat ist wie gewohnt eine dünn besetzte Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = tfidf_transformer.fit_transform(count_vectors)\n",
    "tfidf_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gewichte des Wortes `schaut` sind in den beiden Dokumenten nicht gleich. Das liegt daran, dass TF/IDF-Vektoren normalerweise gleich *normiert* werden. So sind die Gewichte gleicher Wörter nur innerhalb eines Dokuments identisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Genau wie beim `CountVectorizer`kannst du statt der Wörter alleine auch *Bigramme* verwenden. Das Beispiel hier zeigt dir, wie du `CountVectorizer` und `TfidfTransformer` in einem Schritt zusammenfassen kannst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer2 = TfidfVectorizer(ngram_range=(2,2))\n",
    "tfidf_vectors2 = tfidf_vectorizer2.fit_transform([s1, s2])\n",
    "tfidf_vectors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Dokumenten-Term-Matrix stellst du jetzt transponiert dar, weil sie dann einfacher zu lesen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(tfidf_vectors2.todense(), columns=tfidf_vectorizer2.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universelles TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF/IDF wird sehr häufig und für viele Anwendungsfälle eingesetzt - mehr noch als Bag-of-Words. Du wirst dieser Vektorisierung im Laufe des Kurses daher noch häufig begegnen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
