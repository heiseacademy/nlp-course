{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisierung mit regulären Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wörter einfach an den Leerzeichen zu trennen, hat nicht sehr gut funktioniert. Daher wirst du jetzt eine etwas fortgeschrittenere Möglichkeit kennenlernen.\n",
    "\n",
    "Du beginnst wieder mit den bekannten Absätzen aus den [Heise-Neujahrswünschen](https://www.heise.de/news/Guten-Rutsch-und-ein-gesundes-neues-Jahr-2021-5001311.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting mit regulären Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statt einfach nur an Leerzeichen die Worte zu teilen, nutzt du jetzt einen *regulären Ausdruck*. Auch die `regex`-Bibliothek hat eine `split`-Funktion, der kannst du die einzelnen Bedingungen übergeben. Mit `|` verbunden wird an verschiedenen Zeichen geteilt:\n",
    "\n",
    "\n",
    "|Ausdruck|Bedeutung|\n",
    "|:--|:-- |\n",
    "| ` `  | Leerzeichen|\n",
    "| `, ` | Komma gefolgt von Leerzeichen|\n",
    "| `\\. `| Punkt gefolgt von Leerzeichen (ein Punkt ohne `\\` davor steht für ein beliebiges Zeichen)|\n",
    "| `: `| Doppelpunkt gefolgt von Leerzeichen|\n",
    "| `\\.$`| das `$` steht für das Ende einer Zeile und entfernt den letzten Punkt (weil da kein Leerzeichen folgt)|\n",
    "\n",
    "Die Ergebnisse verbindest du wieder mit `|`, damit du die einzelnen Tokens sehen kannst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\"|\".join(re.split(r' |, |\\. |: |\\.$', p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht schon besser aus. Ein paar Tokens stimmen noch nicht ganz, so ist der `-` ein eigener Token und aus `D.C.` ist `D.C` geworden.\n",
    "\n",
    "Betrachte nun den zweiten Absatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(re.split(r' |, |\\. |: |\\.$', p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier ist das Ergebnis ordentlich. Allerdings werden die *regulären Ausdrücke* immer komplizierter, wenn auch noch weitere Satzzeichen mit berücksichtigt werden müssen. Dafür müssen wir einen anderen Weg suchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimierung durch Matching mit regulären Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die zentrale Idee des neuen *regulären Ausdrucks* ist, nicht an einer bestimmte Stelle den Text zu *teilen* (mit `split`), sondern zu definieren, was alles in einem Objekt enthalten sein muss, damit es als Token gilt.\n",
    "\n",
    "Du definierst dazu eine Funktion, die die Zeichenkette tokenisiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'[\\w-]*[[:alpha:]][\\w-]*', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht erst mal unleserlich aus. Wenn du dir den Ausdruck genauer anschaust, siehst du in der Mitte `[[:alpha:]]` stehen. Das muss in jedem Token vorhanden sein und passt auf alle Buchstaben (und zwar in allen möglichen Sprachen). Es ist eine sog. POSIX-Character-Klasse. Davor kann ein Bestandteil eines Worts (`\\w` passt auf Buchstaben, Ziffern und `_`) oder ein `-` in beliebiger Wiederholung stehen, danach auch.\n",
    "\n",
    "Versuche jetzt, die Funktion anzuwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(tokenize(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis sieht gut aus, allerdings hat es auch einie Schwächen. So wurden aus `D.C.` die Tokens `D` und `C` extrahiert, was natürlich nicht ganz richtig ist.\n",
    "\n",
    "Probiere es noch mit dem zweiten Text aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(tokenize(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist nicht ganz optimal, weil c't mehrfach falsch geteilt wurde. Ansonsten ist fast alles richtig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisierung ist schwierig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du gesehen hast, ist Tokenisierung ein ziemlich schwieriges Problem schon in unserem Sprachraum. Noch sehr viel komplexer wird es bei ostasiatischen Sprachen (*CJK*: Chinesisch, Koreanisch, Japanisch), weil diese gar keine Leerzeichen verwenden und Wortgrenzen auch nicht optisch erkennbar machen.\n",
    "\n",
    "Zum Glück musst du dich normalerweise nicht damit beschäftigen, weil es bereits fertig Tokenisierer für fast alle Sprachen gibt, die wir uns in der nächsten Lektion anschauen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
