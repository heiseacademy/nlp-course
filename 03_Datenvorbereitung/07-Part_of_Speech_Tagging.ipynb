{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach und nach wollen wir in den nächsten Schritte eine *Verarbeitungspipeline* aufbauen. Mit der Tokenisierung hast du bereits einen entscheidenen Schritt hinter dir. Die richtige Landessprache kennst du auch und kannsst daher das richtige Sprachmodell wählen.\n",
    "\n",
    "Als nächstes wirst du jetzt den Satzbau analysieren, Wortarten und Morphologie bestimmen. Schließlich geht es über die Abhängigkeitsanalyse zur Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistische Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Beispiel für die folgenden Aufgaben nutzt du zwei Absätze aus dem Neujahrs-Artikel von Heise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie schon in den vorherigen Kapitel verwendest du `spacy` für die Analyse. Das Modell `de_core_news_lg` wurde mit dem sog. *TIGER Corpus* und *WikiNER* trainiert, so dass es mit ausreichender Genauigkeit arbeiten kann.\n",
    "\n",
    "Den Code für die Analyse kennst du schon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "d1 = nlp(p1)\n",
    "d2 = nlp(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun interessieren wir uns nicht für die einzelnen Tokens alleine, sondern für weitergehende Informationen zu den Tokens. Jeder Token wird von `spacy` mit zusätzlichen Informationen angereichert. Dazu schreibst du nun eine kleine Funktion, die das als Pandas `DataFrame` ausgibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max.rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def pos_tag(d):\n",
    "    res = []\n",
    "    for token in d:\n",
    "        res.append([token.text, token.pos_, token.tag_, token.lemma_, token.dep_ ])\n",
    "\n",
    "    return pd.DataFrame(res, columns=[\"Text\", \"POS\", \"Tag\", \"Lemma\", \"Dep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion kannst du nun auf das erste `spacy`-Dokument anwenden, womit sich eine umfangreiche Tabelle ergibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1 = pos_tag(d1)\n",
    "pt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`POS` ist dabei der Part-of-Speech-Tag: \n",
    "\n",
    "|POS|Bedeutung|\n",
    "|:--|:--|\n",
    "|`ADJ`|Adjektiv|\n",
    "|`ADP`|Adposition|\n",
    "|`ADV`|Adverb|\n",
    "|`AUX`|Hilfsverb|\n",
    "|`CONJ`|Gleichordnende Konjunktion|\n",
    "|`DET`|Artikel|\n",
    "|`INTJ`|Ausruf|\n",
    "|`NOUN`|Substantiv|\n",
    "|`NUM`|Zahl|\n",
    "|`PART`|Partikel|\n",
    "|`PRON`|Pronomen|\n",
    "|`PROPN`|Substantiv|\n",
    "|`PUNCT`|Punktuation|\n",
    "|`SCONJ`|unterordnende Konjunktion|\n",
    "|`SYM`|Symbol|\n",
    "|`VERB`|Verb|\n",
    "\n",
    "Quelle: https://universaldependencies.org/docs/u/pos/\n",
    "\n",
    "`Tag` ist deutlich detaillierter und sowohl spezifisch für `spacy` als auch für das jeweilige Modell. Details dazu findest du unter https://spacy.io/models/de. \n",
    "\n",
    "Du kannst dir Erklärungen außerdem mit `spacy.explain` ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = []\n",
    "for tag in pt1[\"Tag\"].unique():\n",
    "    ex.append({\"Tag\": tag, \"Explanation\": spacy.explain(tag)})\n",
    "pd.DataFrame(ex).sort_values(\"Tag\").set_index(\"Tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `POS` und `Tag` kannst du viele Wörter deutlich besser *qualifizieren*. Dieses Wissen wirst du in den folgenden Kapiteln anwenden, um z.B. wortartenspezifische Auswertungen durchzuführen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
