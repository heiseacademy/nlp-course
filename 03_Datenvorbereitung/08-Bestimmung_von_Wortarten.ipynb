{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "d1 = nlp(p1)\n",
    "d2 = nlp(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max.rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def tag_morph(d):\n",
    "    res = []\n",
    "    for token in d:\n",
    "        res.append([token.text, token.tag_, token.pos_, str(token.morph)])\n",
    "\n",
    "    return pd.DataFrame(res, columns=[\"Text\", \"Tag\", \"POS\", \"Morphologie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm1 = tag_morph(d1)\n",
    "tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substantive\n",
    "tm1[(tm1[\"POS\"] == \"NOUN\") | (tm1[\"POS\"] == \"PROPN\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjektive\n",
    "tm1[(tm1[\"POS\"] == \"ADJ\") | (tm1[\"POS\"] == \"ADV\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verben\n",
    "tm1[(tm1[\"POS\"] == \"VERB\") | (tm1[\"POS\"] == \"AUX\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_types(doc):\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "        # adjective\n",
    "        if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "            adjectives.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "        # noun\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            nouns.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "        # verb\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "            \n",
    "    return (nouns, adjectives, verbs, nav, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(nouns, adjectives, verbs, nav, lemmas) = word_types(d1)\n",
    "print(f'Substantive: {\"|\".join(nouns)}')\n",
    "print(f'Adjektive: {\"|\".join(adjectives)}')\n",
    "print(f'Verben: {\"|\".join(verbs)}')\n",
    "print(f'Kombinationen: {\"|\".join(nav)}')\n",
    "print(f'Lemmas: {\"|\".join(lemmas)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
