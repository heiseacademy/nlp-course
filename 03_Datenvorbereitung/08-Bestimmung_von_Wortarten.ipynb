{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bestimmung von Wortarten und Morphologie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wortarten spielen nicht umsonst eine sehr große Rolle in der Sprache. Aber auch die Art und Weise, wie Wörter verwendet werden, trägt erheblich zur Kommunikation bei - du kennst vielleicht noch den *Ich-Erzähler* aus der Schule.\n",
    "\n",
    "Zum Glück können alle diese Analysen auch von `spacy` durchgeführt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basisdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt beginnst du wieder mit den beiden Paragraphen aus dem Neujahrs-Artikel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `spacy` analysierst du die Texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "d1 = nlp(p1)\n",
    "d2 = nlp(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der *Tag* ist ziemlich präzise, außerdem lässt du dir noch *POS* und die *Morphologie* ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max.rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def tag_morph(d):\n",
    "    res = []\n",
    "    for token in d:\n",
    "        res.append([token.text, token.tag_, token.pos_, str(token.morph)])\n",
    "\n",
    "    return pd.DataFrame(res, columns=[\"Text\", \"Tag\", \"POS\", \"Morphologie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm1 = tag_morph(d1)\n",
    "tm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Morphologie ist relativ neu in `spacy` integriert und steht ab Version `3.0` zur Verfügung. Die darin enthaltenen Informationen kannst du z.B. nutzen, um eine bestimmte *Textart* zu erkennen (und später zu klassifizieren). Auch der Schreibstil ist implizit in der Morphologie enthalten, weil über `Mood` und `VerbForm` viel Information transportiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS und Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`POS` und `Tag` drücken jeweils *Wortarten* aus, `Tag` ist dabei allerdings genauer. An Beispielen siehst du die Unterschiede gut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substantive\n",
    "tm1[(tm1[\"POS\"] == \"NOUN\") | (tm1[\"POS\"] == \"PROPN\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjektive\n",
    "tm1[(tm1[\"POS\"] == \"ADJ\") | (tm1[\"POS\"] == \"ADV\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verben\n",
    "tm1[(tm1[\"POS\"] == \"VERB\") | (tm1[\"POS\"] == \"AUX\")][\"Tag\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt zerlegst du nun den Text in *Substantive*, *Adjektive* und *Verben*, von denen du dir nur die Grundform merkst. In `nav` speicherst du diese Wortarten in der richtigen Reihenfolge ab. Da du dies häufiger durchführen musst, bietet es sich an, das in einer Funktion zu kapseln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_types(doc):\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "        # adjective\n",
    "        if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "            adjectives.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "        # noun\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            nouns.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "        # verb\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.lemma_)\n",
    "            nav.append(token.lemma_)\n",
    "            \n",
    "    return (nouns, adjectives, verbs, nav, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(nouns, adjectives, verbs, nav, lemmas) = word_types(d1)\n",
    "print(f'Substantive: {\"|\".join(nouns)}')\n",
    "print(f'Adjektive: {\"|\".join(adjectives)}')\n",
    "print(f'Verben: {\"|\".join(verbs)}')\n",
    "print(f'Kombinationen: {\"|\".join(nav)}')\n",
    "print(f'Lemmas: {\"|\".join(lemmas)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du dir die Ergebnisse ansiehst, wirst du feststellen, dass die Substantive und Verben sehr gut erkannt wurden. Einige Adjektive sind in Wahrheit allerdings Substantive, d.h. damit hat `spacy` ab und zu noch Schwierigkeiten. Wenn du magst, tausche das Modell aus und übrprüfe, ob sich dadurch Verbesserungen ergeben."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
