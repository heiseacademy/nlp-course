{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unter der Tokenisierung versteht man das Aufteilen eines Textes in seine Bestandteile, die sog. *Tokens*. In den allermeisten Fällen werden die Tokens die einzelnen Wörter sein, da kann es aber Ausnahmen geben.\n",
    "\n",
    "Für die Tokenisierung betrachten wir zwei Absätze aus den [Heise-Neujahrswünschen](https://www.heise.de/news/Guten-Rutsch-und-ein-gesundes-neues-Jahr-2021-5001311.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menschliche Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten wir zuerst den ersten Absatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du den Text in Wörter (oder evtl. auch in einzelne Sätze) aufteilen möchtest, musst du dir mehrere Fragen stellen:\n",
    "* Wo enden Wörter? <br>\n",
    "  In den meisten Fällen ist das relativ eindeutig, oft aber auch nicht. *D.C.* sollte z.B. ein einzelnes Wort sein, *Covid-19* auch\n",
    "  \n",
    "* Wo enden Sätze?<br>\n",
    "  Ein Punkt kann ein Ende eines Satzes beudeuten, das muss aber nicht zwingend so sein. *D.C.* ist ein Beispiel, in dem du dich nicht auf den Punkt verlassen kannst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der zweite Absatz ist auch nicht viel einfacher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier gibt es sehr viele Eigennamen:\n",
    "* *c't* ist ein Wort und im Vergleich z.B. zu *hat's* darf das Wort nicht in Tokens aufgteilt werden\n",
    "* Oft ist ein *:* ein Zeichen für ein Satzende, das stimmt hier aber bei *Make:* nicht.\n",
    "* Ideal wäre es, wenn wir Eigennamen auch als Tokens erkennen könnten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versuch einer einfachen Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst jetzt versuchen, die Dokumente automatisch zu tokenisieren. Der einfachste Ansatz ist, einfach die Wörter an Leerzeichen zu trennen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(p1.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(p2.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ganz schlecht hat das nicht funktioniert. Allerdings sind jetzt noch oft Satzzeichen in den Wörtern selbst enthalten. Um diese zu entfernen, brauchst du die schon bekannten *regulären Ausdrücke*, die wir im nächsten Teil betrachten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
