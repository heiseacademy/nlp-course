{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisierung mit `spacy` und `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den letzten Kapiteln haben wir gesehen, dass die Tokenisierung von Texten nicht mit ganz einfachen Mitteln zu lösen ist. Allerdings kannst du im Python-Ökosystem auf eine große Vielfalt an Tools zurückgreifen, die u.a. auch tokenisieren können.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\"\n",
    "\n",
    "p2 = \"Wir, das gesamte Team von heise online und die Redaktionen von c't, iX, \\\n",
    "Technology Review, Mac & i, c't Digitale Fotografie, Make:, Techstage und \\\n",
    "Telepolis sowie heise Security, heise Developer und heise Autos wünschen Ihnen \\\n",
    "ein friedliches und freudvolles Jahr 2021. Wir wünschen Ihnen, dass Sie nicht \\\n",
    "vergeblich hoffen und dass Ihre Vorsätze erfüllt werden, auf dass Sie gesund \\\n",
    "bleiben oder genesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisierung mit `spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spacy` hast du schon in der Einleitung kurz kennengelernt. Es hat sich in den letzten Jahren als Standard-Tool zur linguistischen Analyse durchgesetzt.\n",
    "\n",
    "Als eine Vorstufe dazu führt es auch die Tokenisierung durch. Um Zeit bei umfangreichen Dokumenten oder vielen Aufrufen zu sparen, kannst du in der *Pipeline* die Folgeschritte nach der Tokenisierung mit `disable=['tagger', 'parser', 'ner']` abschalten. Da wir uns auch für Sätze und *Entitäten* interessieren, machen wir das hier nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "d1 = nlp(p1)\n",
    "d2 = nlp(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit hast du die Dokumente durch `spacy` analysieren lassen und dabei einige Schritte übersprungen (die werden wir später noch besprechen). Jetzt kannst du überprüfen, wie die Tokenisierung geklappt hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"|\".join([str(d) for d in d1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spacy` hat seine Sache sehr gut erledigt. Beacht, dass Satzzeichen in `spacy` eigene Tokens sind, die auch entsprechend gekennzeichnet werden. Daher sind nicht nur Wörter in den Tokens enthalten.\n",
    "\n",
    "`spacy` kennt außerdem sog. *Entitäten*, also zusammenhängende Wortkombinationen. Überprüfe, ob es im Text welche gefunden hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spacy` kann auch Sätze voneinander trennen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d1.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schau dir zum Vergleich noch das zweite Dokument an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join([str(d) for d in d2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist bei der Tokenisierung ein subtiler Fehler passiert. `2021.` wurde als ein Token identifiziert. Du kannst überprüfen, ob das auch Auswirkungen auf die erkannt Satzstruktur hat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d2.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Tat hat das nicht perfekt funktioniert. Für eine spätere Analyse wäre das wahrscheinlich unproblematisch, aber du kannst sehen, dass diese Analyse nicht immer perfekt funktioniert.\n",
    "\n",
    "Der Satz enthält durch die Namen der Publikationen viele Entitäten. Du kannst sie dir ausgeben lassen und nachsehen, ob alle erkannt worden sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das hat sehr gut funktioniert. In den nächsten Lektionen betrachten wir, wie du mit diesen Entitäten umgehen kannst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisierung mit `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben `spacy` wird zur linguistischen Analyse auch das etwas ältere `nltk` eingesetzt. Hier findest du ebenso eine `tokenize`-Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzliche Daten für NLTK herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die eigentliche Tokenisierung startet hier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\"|\".join(tokenize.word_tokenize(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist sehr ähnlich wie das von `spacy`. Auch Sätze kannst du damit erkennen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.sent_tokenize(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das zweite Dokument funktioniert sogar etwas besser als mit `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(tokenize.word_tokenize(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2021` wurde hier richtig erkannt - das liegt aber eher daran, dass `nltk` keine *Ordinalzahlen* erkennen kann. Damit ist auch die Aufteilung in Sätze genau richtig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.sent_tokenize(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` ist ein leistungsfähiges Software-Paket, aber nicht so gut auf die Mehrsprachfähigkeit wie `spacy` ausgelegt. Besonders die Folgeaufgaben (Part-of-Speech etc.) kann es nicht so gut bewältigen wie `spacy`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
