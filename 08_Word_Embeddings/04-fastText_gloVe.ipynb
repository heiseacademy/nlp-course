{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText und glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Embeddings kennst du jetzt *word2vec*. Es gibt allerdings noch Alternativen dazu.\n",
    "\n",
    "Von Facebook wurde [fastText](https://fasttext.cc/) \"erfunden\". *fastText* geht einen etwas anderen Weg als *word2vec* und nutzt auch noch Buchstaben-n-Gramme zum Training. Dadurch werden auch syntaktisch ähnliche Wörter gefunden und es ist ziemlich stabil gegen Tippfehler, weswegen es sich auch zur Landesspracherkennung eignet (die hast du schon kennengelernt). *fastText* bietet dir außerdem bereits sehr viele vortrainierte Modell zum Download an und ist damit ein sehr komplettes Paket.\n",
    "\n",
    "Parallel zu *word2vec* bei Google wurde in Stanford [gloVe]() entwickelt. *gloVe* wird über eine Co-Occurrence-Matrix trainiert und produziert daher signifikant andere Resultate. Leider lässt sich *gloVe* nicht in Python trainieren, so sind wir auf ein CLI-Programm ausgewichen und stellen die Embeddings bereit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gensim>=4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText Embeddings trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kennst schon die Datenvorbereitung für `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "gensim_words = [[w for w in re.split(r'[\\\\|\\\\#]', doc.lower()) if w not in stop_words]\n",
    "                    for doc in df[\"nav\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastText` wird  genauso trainiert wie `Word2Vec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "ft = FastText(gensim_words, min_count=5)\n",
    "ft.wv.save_word2vec_format(\"heise-articles-2020.ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und auch das API ist identisch. Hier siehst du bereits die Effekter der Buchstaben-n-Gramme: `jva` würde bei `Word2Vec` keine hohe Ähnlichkeit haben, ebenso nicht die ganzen Begriffe, die `javascript` zuzuordnen sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.wv.most_similar(positive=[\"java\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei `microsoft` und `corona` ist der Effekt noch ausgeprägter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.wv.most_similar(positive=[\"microsoft\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.wv.most_similar(positive=[\"corona\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analogieschlüsse funktionieren aber ähnlich gut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ft.wv.most_similar(positive=[\"apple\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ft.wv.most_similar(positive=[\"microsoft\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst musst du dazu das File `heise-articles-nav-2020.txt` erzeugen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"heise-articles-nav-2020.txt\", \"w\").write(\"\\n\".join([n.lower() \n",
    "                                                          for n  in df[\"nav\"].str.replace(r'\\#|\\|', ' ').values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider gibt es in Python keine direkte Möglichkeit, GloVe-Embeddings zu trainieren. Wir haben sie daher mit folgendem Skript vortrainiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "VOCAB_FILE=vocab.txt\n",
    "COOCCURRENCE_FILE=cooccurrence.bin\n",
    "COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin\n",
    "BUILDDIR=build\n",
    "SAVE_FILE=heise-articles-2020.glove\n",
    "VERBOSE=2\n",
    "MEMORY=4.0\n",
    "VOCAB_MIN_COUNT=5\n",
    "VECTOR_SIZE=50\n",
    "MAX_ITER=15\n",
    "WINDOW_SIZE=15\n",
    "BINARY=2\n",
    "NUM_THREADS=8\n",
    "X_MAX=10\n",
    "CORPUS=heise-articles-nav-2020.txt\n",
    "BUILDDIR=~/src/GloVe-1.2/build/\n",
    "$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\n",
    "$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE <$CORPUS > $COOCCURRENCE_FILE\n",
    "$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\n",
    "$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter 100 -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das resultierende File kannst du direkt in `gensim` einladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_COLAB:\n",
    "    os.system(\"heise-articles-2020.glove.txt || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.glove.txt.gz && gunzip heise-articles-2020.glove.txt.gz\")\n",
    "    glove_file = 'heise-articles-2020.glove.txt'\n",
    "else:\n",
    "    glove_file = '../99_Common/heise-articles-2020.glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "glove = KeyedVectors.load_word2vec_format(glove_file, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am besten vergleichst du die Ergebnisse mit denen von `FastText` oben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive=[\"java\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist deutlich konzeptioneller!\n",
    "\n",
    "Wie sieht es bei `Microsoft` aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive=[\"microsoft\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einige relativ allgemeine Wörter tauchen plötzlich auf. Aber sonst sieht das Ergebnis prima aus. Ebenso ist es bei `Corona`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive=[\"corona\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analogieschlüsse funktionieren noch besser als bei `Word2Vec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glove.most_similar(positive=[\"apple\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(glove.most_similar(positive=[\"microsoft\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText und Glove nach Anwendungsfall auswählen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*FastText* konzentriert sich auf neben der semantischen Ähnlichkeit besonders auf die Klassifikation *unscharfer Daten* und die Identifikation von Wörtern, die ähnlich geschrieben werden. *FastText* ist perfekt in `gensim` eingebunden und kann genau wie `Word2Vec` verwendet werden.\n",
    "\n",
    "*GloVe* hingegen arbeitet deutlich konzeptioneller und ist leider nicht direkt als Python-Bibliothek vorhanden. Dadurch ist wahrscheinlich auch seine geringere Popularität zu erklären, die Ergebnisse sind nämlich (abgesehen von einigen generischen Wörtern) sehr gut."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
