{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich und Visualisierung der Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast jetzt unterschiedliche Embeddings kennengelernt. Jetzt möchtest du deren Ergebnis miteinander vergleichen. Dazu gibt es verschiedene Visualisierungen, du wirst dir dazu zwei ganz unterschiedliche grafische Darstellungen anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit du die Embeddings nicht nochmal berechnen musst, lädst du die vorher gespeicherten Wortvektoren einfach ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gensim>=4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.w2v || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.w2v.gz && gunzip heise-articles-2020.w2v.gz\")\n",
    "    os.system(\"test -f heise-articles-2020.ft || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.ft.gz && gunzip heise-articles-2020.ft.gz\")\n",
    "    os.system(\"test -f heise-articles-2020.glove.txt || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.glove.txt.gz && gunzip heise-articles-2020.glove.txt.gz\")\n",
    "    w2v_file = \"heise-articles-2020.w2v\"\n",
    "    ft_file = \"heise-articles-2020.ft\"\n",
    "    glove_file = \"heise-articles-2020.glove.txt\"\n",
    "else:\n",
    "    w2v_file = \"../99_Common/heise-articles-2020.w2v\"\n",
    "    ft_file = \"../99_Common/heise-articles-2020.ft\"\n",
    "    glove_file = \"../99_Common/heise-articles-2020.glove.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = KeyedVectors.load_word2vec_format(ft_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(glove_file, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D-Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Visualisierung kannst du dir die Word Embeddings leider nicht alle ausgeben lassen, denn das sind ziemlich viele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stattdessen kannst du dich auf die Keywords konzentrieren, am besten auch nur auf die Top-Keywords. Die Berechnung der Häufigkeiten kennst du schon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT keywords FROM articles WHERE datePublished<'2021-01-01'\", sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keywords = Counter([keyword for keywords in df[\"keywords\"] for keyword in str(keywords).split(\", \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabei solltest du darauf achten, dass keine Leerzeichen enthalten sind, weil die bei den Embeddings als unterschiedliche Wörter interpretiert wurden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = [k[0] for k in keywords.most_common(100) if not \" \" in k[0]][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt hast du die Anzahl der Embeddings reduziert - aber leider haben sie noch immer 300 Dimensionen. Eine sinnvolle Darstellung kannst du damit nicht erreichen. Allerdings kannst du die [Anzahl der Dimensionen reduzieren](https://en.wikipedia.org/wiki/Dimensionality_reduction). Dafür gibt es verschiedene Verfahren, hier benutzt du [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "\n",
    "Zunächst berechnest du eine Matrix mit (möglichst) allen Embeddings der Keywords. Dabei können einzelne fehlen, die durch Exception-Handling dann leer sind. Anschließend führst du mit `TSNE` eine Dimensionsreduktion durch und stellst das Ergebnis in zwei Dimensionen als Scatter-Plot dar. Schließlich muss der Scatter-Plot noch beschriftet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_wv(model, words):\n",
    "    wr = []\n",
    "    wk = []\n",
    "    for w in words:\n",
    "        # manche Wörter könnten fehlen\n",
    "        try:\n",
    "            wr.append(model[w.lower()])\n",
    "            wk.append(w)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42).fit_transform(wr)\n",
    "    tsne_df = pd.DataFrame(tsne, columns = [\"x\", \"y\"])\n",
    "    \n",
    "    ax = tsne_df.plot.scatter(x='x', y='y', figsize=(16, 9))\n",
    "\n",
    "    for i, txt in enumerate(wk):\n",
    "        ax.annotate(txt, (tsne_df.x[i], tsne_df.y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachte nun, wie sich die `word2vec`-Embeddings verhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wv(w2v, top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zum Vergleich dazu die von `fastText`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wv(ft, top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glove` ordnet etwas anders und packt die Länder enger zusammen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wv(glove, top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über diese zweidimensionalen Darstellungen kannst du dir einen Überblick verschaffen, wie die Wörter in den einzelnen Embeddings zueinander stehen. Beachte dabei, dass als Ähnlichkeitsmaß normalerweise der Winkel zwischen Wörtern verwendet wird, der hier durch die Dimensionsreduktion sicher sehr verzerrt wurde!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlichkeiten kannst du auch ganz anders darstellen - nämlich als Netzwerk, bei dem du von einem zentralen Wort ausgehst, zzu diesem die ähnlichsten Wörter suchst usw. Dadurch ergibt sich ein *Graph*.\n",
    "\n",
    "Python hat mit `networkx` eine ziemlich flexible Bibliothek zur Verarbeitung und auch zur Darstellung von Graphen. Dazu schreibst du dir zunächst eine Funktion, die den Graph mit den Wörtern als Knoten aufbauen und die Kanten zwischen ähnlichen Wörtern aufspannt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def graph(model, w0):\n",
    "    G=nx.Graph()\n",
    "    G.add_node(w0)\n",
    "    for (w1, s1) in model.most_similar(w0):\n",
    "        G.add_node(w1)\n",
    "        G.add_edge(w0, w1, weight=s1)\n",
    "        for (w2, s2) in model.most_similar(w1):\n",
    "            G.add_node(w2)\n",
    "            G.add_edge(w1, w2, weight=s2)\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachtet nun die Ähnlichkeiten von `apple` om Graph mit `word2vec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "G = graph(w2v, \"apple\")\n",
    "nx.draw_kamada_kawai(G, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im  Vergleich mit `fastText` fällt auf, dass hier viel mehr Ähnlichkeiten gefunden werden, die auf ähnliche Schreibweisen zurückzuführen sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "G = graph(ft, \"apple\")\n",
    "nx.draw_kamada_kawai(G, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei `glove` sieht schon die Form ganz anders aus. Wie du siehst, sind deutlich mehr allgemeine Wörter enthalten, die wenig spezifische Bedeutung transportieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "G = graph(glove, \"apple\")\n",
    "nx.draw_kamada_kawai(G, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings verhalten sich unterschiedlich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst verhalten sich die Embeddings sehr verschieden. `word2vec` fängt eher konzeptionelle Ähnlichkeiten ein, während `fastText` sich mehr auf Syntax konzentriert. `glove` produziert teilweise sehr gute Resultate, teilweise aber auch nur ganz allgemeine Wörter.\n",
    "\n",
    "Welches Embedding für dich am besten geeignet ist, musst du anhand deiner Anforderungen selbst herausfinden. Die oben gezeigten Visualisierungen können dich dabei unterstützen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
