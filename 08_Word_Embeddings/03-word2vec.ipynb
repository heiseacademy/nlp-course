{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings mit `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher hast du Machine Learning mit Dokumenten durchgeführt. Ähnlichkeiten von Wörtern oder Mehrwort-Kombinationen haben dabei keine Rolle gespielt.\n",
    "\n",
    "Jetzt versuchst du, eine Darstellung von Wörtern zu finden, mit denen du auch Ähnlichkeiten zwischen ihnen berechnen kannst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gensim>=4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings berechnest du am besten mit `gensim`. Wiew gewohnt musst du dazu die Dokumente in ein doppelt geschachteltes Array transformieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "gensim_words = [[w for w in re.split(r'[\\\\|\\\\#]', doc.lower()) if w not in stop_words]\n",
    "                    for doc in df[\"nav\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend kannst du das Embedding ausrechnen. Hier ist es so eingestellt, dass nur Wörter berücksichtigt werden, die mindestens fünfmal vorkommen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(gensim_words, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Wortvektoren auch später wieder verwenden zu können, speicherst du sie ab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.save_word2vec_format(\"heise-articles-2020.w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ähnlichkeiten abfragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du mit einfachen Ähnlichkeitsabfragen starten. Wenn dich interessiert, was ähnlich zu `java` ist, kannst du folgende Abfrage ausprobieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=[\"java\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist absolut vernünftig und enthät sowohl konzeptionelle Ähnlichenkeiten (andere Sprachen wie Python und Javascript), aber auch damit verwandte Konzepte (wie Programmiersprache und Entwicklerteam).\n",
    "\n",
    "Probier es nochmal mit `microsoft` aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=[\"microsoft\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoffentlich ist `Corona` kein Thema mehr, wenn du das liest. Im Moment leider schon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=[\"corona\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec` kann wie alle Embeddings auch mit Addition und Subtraktion der Vektoren arbeiten und daraus Analogieschlüsse ziehen.\n",
    "\n",
    "Betrachte dazu folgende Gleichung:\n",
    "\n",
    "Android - Google = ? - Apple\n",
    "\n",
    "Durch Umformung kannst du nach `?` auflösen:\n",
    "\n",
    "? =  Apple + Android - Google\n",
    "\n",
    "Welche Ergebnis liefert `Word2Vec`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w2v.wv.most_similar(positive=[\"apple\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erwartet hätte man `ios`, aber auch die anderen Ergebniss sind schon sehr nah an der richtigen Lösung, weil mit Android durchaus auch Geräte gemeint sein können.\n",
    "\n",
    "Wenn dir entfallen sein sollte, wie Microsofts Betriebssystem heißt, kannst du `Word2Vec` fragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w2v.wv.most_similar(positive=[\"microsoft\", \"android\"], \n",
    "                                   negative=[\"google\"],  topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist absolut richtig - und das wurde alles ohne Überwachung trainiert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrasen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` kann in dem Korpus auf Phrasen identifizieren und diese statt den Tokens als Vokabular für das Training verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "entity_transformer = Phrases(gensim_words)\n",
    "w2vp = Word2Vec(entity_transformer[gensim_words], min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse für `corona` unterscheiden sich nicht sehr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vp.wv.most_similar(positive=[\"corona\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei `java` kannst du nun einige Phrasen erkennen, die durch `_` markiert sind. es ist schwer zu entscheiden, ob das Ergebnis wirklich besser ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vp.wv.most_similar(positive=[\"java\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Word2Vec* als häufigste Anwendung von Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig wird *Word2Vec* nahezu synonym mit Word Embeddings verwendet - zugegebenermaßen war es auch das erste Word Embedding, was es zu einer signifikanten Verbreitung gebracht hat. Die Performance ist dabei ziemlich gut, sowohl die Berechnung als auch die Abfrage kannst du in (Nahezu-) Echtzeit durchführen (die Abfrage ist bei allen Embeddings gleich schnell).\n",
    "\n",
    "Anwendungsfallabhängig ist es daher eine gute Idee, wenn du zunächst mit Word2Vec startest und das als Baseline benutzt. Wenn dir die anderen Embeddings, die du später kennenlernen wirst, keine Vorteile bringen, bleibst du einfach bei Word2Vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
