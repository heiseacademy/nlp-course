{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Überblick über Basis-Tools: `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` ist das Standard-Tool für Machine Learning in Python. Fast alle *klassischen ML-Verfahren* werden durch `scikit-learn` unterstützt.\n",
    "\n",
    "`scikit-learn` verfügt (im Gegensatz zu vielen anderen Open-Source-Paketen) über eine fantatische Dokumentation, die du dir unter https://scikit-learn.org/stable/ unbedingt anschauen solltest.\n",
    "\n",
    "Sehr praktisch ist auch, dass `scikit-learn` bereits Datensets mitbringt, mit denen man die Funktionalität ausprobieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als ganz einfaches Beispiel nutzen wir hier ein Datenset, das aus 20 Newsgroups besteht. Es ist zwar schon ziemlich alt, aber die Sprache hat sich nicht sehr geändert. Das Datenset lässt sich ganz leicht laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten findest du in dem Schlüssel `data` des zurückgelieferten `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und jetzt kannst du fast schon mit dem Machine Learning beginnen. Die Daten müssen nur noch *vektorisiert* werden. Was dabei genau passiert, schauen wir uns später noch an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobald du die Vektoren zur Verfügung hast, kannst du einen Klassifikator trainieren, hier eine sog. *Support Vector Machine*. Der Klassifikator soll lernen, aufgrund eines Newsgroups-Posting die korrekte Kategorie (in diesem Fall die Newsgroup, in der gepostet wurde) zu erraten. Ein solches Problem heißt *Klassifikationsproblem*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "clf.fit(vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ging **schnell**. Immerhin waren es über 11.000 Texte, aus denen das Modell die Kategoriezuordnung gelernt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand einer unabhängigen Datenmenge kannst die Ergebnisse verfizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch das geh sehr schnell und mit über 84% *F-Score* ziemlich genau. Wenn du dir anschauen möchtest, wo Fehler passiert sind, hilft die sog. *Confusion Matrix*, die in der Diagonale die richtigen Ergebnisse zeigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(metrics.confusion_matrix(newsgroups_test.target, pred), \n",
    "             columns=newsgroups_train.target_names, index=newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verwechslungen von `alt.atheism` mit `talk.religion.misc` und `soc.religion.christian` kann man in Nicht-Diagonal-Elementen der Confusion Matrix gut erkennen. Die könnten einem Menschen auch passieren, genau wie `comp.os.ms-windows.misc` mit `comp.sys.ibm.pc.hardware` und `comp.windows.x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` kann nebem *überwachtem Lernen* auch viel Funktionen zum *unüberwachten Lernen* vorweisen. Unüberwachtes Lernen kannst du dazu nutzen, die organische Struktur eines Textkorpus zu explorieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=5)\n",
    "nmf.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "topics = []\n",
    "for topic, word_vector in enumerate(nmf.components_):\n",
    "    total = word_vector.sum()\n",
    "    largest = word_vector.argsort()[::-1] # invert sort order\n",
    "    \n",
    "    topics.append([f\" {features[largest[i]]}\" for i in range(5)])\n",
    "pd.DataFrame(topics, columns=[f\"Wort {i}\" for i in range(5)], index=[f\"Topic {i}\" for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier siehst du, dass wir noch mehr Arbeit in die Vorbereitung der Daten stecken müssen. Die wichtigsten Worte der Topics sind fast alle ziemlich bedeutungslos (sog. *Stoppworte*). Lösungen dafür schauen wir uns im Laufe des Kurses an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` kann noch viel mehr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast bisher nur an der Oberfläche von `scikit-learn` gekratzt. `scikit-learn` bietet noch viel mehr Algorithmen,\n",
    "Methoden und Konzepte, die wir uns in späteren Lektionen genau ansehen werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
