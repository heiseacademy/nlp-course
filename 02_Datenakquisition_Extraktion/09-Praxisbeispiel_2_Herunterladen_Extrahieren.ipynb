{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8\tPraxisbeispiel Teil 2: Dateien herunterladen und extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Schritt hast du gesehen, wie wir die URLs des Heise Newstickers extrahieren können. Nun werden die die einzelnen Seiten herunterladen und extrahieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seiten herunterladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Websiten zu akquirieren, lädst du im ersten Schritt die URLs ein, die du zuvor generiert hast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f urls-2020.txt || wget https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/urls-2020.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = open(\"urls-2020.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am besten speicherst du alle heruntergeladenen Artikel in einem eigenen Verzeichnis. Damit du das Notebook öfter laufen lassen kannst, legest du das Verzeichnis nur an, wenn es nicht bereits existiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir(\"artikel\"):\n",
    "    os.mkdir(\"artikel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei größeren Downloads solltest du außerdem versuchen, nur Dateien herunterzuladen, die du nicht vorher schon heruntergeladen hattest. Dazu legst du am besten eine Funktion an, die aus einer URL einen Dateinamen konstruiert. Bei den Heise-Artikeln kannst du einfach den Teil der URL nach dem letzten `/` nehmen, grundsätzlich wären aber auch MD5-Hashes der URLs möglich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_filename(url):\n",
    "    c = url.split(\"/\")\n",
    "    return \"/\".join([\"artikel\", c[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Schleife lädst du nun alle Seiten herunter, die noch nicht in dem Artikel-Verzeichnis existieren. Um von HTTP-Keepalive profitieren zu können, nutzt du die `Session` von `requests`. Trotzdem kann der Download eine ganze Weile dauern. Damit du immer siehst, wie der Fortschritt ist, kapselst du den Iterator in `tqdm`, einer sehr schönen Forschrittsanzeige:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests.Session()\n",
    "for u in tqdm(urls):\n",
    "    if u == '':\n",
    "        continue\n",
    "    filename = article_filename(u)\n",
    "    if not os.path.isfile(filename):\n",
    "        print(filename)\n",
    "        print(u)\n",
    "        r = s.get(\"https://www.heise.de\" + u)\n",
    "        open(filename, 'wb').write(r.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tqdm` nutzt du auch am besten für die Extraktion. Dazu kannst du einfach den Code aus *Lektion 6* übernehmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "res = []\n",
    "for u in tqdm(urls):\n",
    "    filename = article_filename(u)\n",
    "    html = open(filename).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    d = {}\n",
    "    d[\"title\"] = soup.h1.text.strip()\n",
    "    d[\"header\"] = soup.select_one(\"#meldung > div.article-layout__header-container > header > p\").text.strip()\n",
    "    d[\"author\"] = soup.select_one(\"a.redakteurskuerzel__link\").attrs[\"title\"]\n",
    "    d[\"text\"] = \"\\n\".join([p.text.strip() \n",
    "                            for p in soup.select(\"#meldung > div.article-layout__content-container > div > p\")])\n",
    "    d[\"keywords\"] = soup.find(\"meta\", {\"name\": \"keywords\"})[\"content\"]\n",
    "    ld = json.loads(soup.find(\"script\", type=\"application/ld+json\").string)\n",
    "    for k in [\"identifier\", \"url\", \"datePublished\", \"commentCount\"]:\n",
    "        d[k] = ld[0][k]\n",
    "    \n",
    "    res.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was mit einer Seite funktioniert, muss leider noch lange nicht für alle Seiten funktionieren...\n",
    "\n",
    "Offenbar haben nicht alle Seiten einen Autor, auch ein Header ist nicht auf allen Seiten vorhanden. Wenn du weiter probierst, wirst du sehen, dass auch `ld+json`-Informationen nicht überall zu finden sind.\n",
    "\n",
    "Um das richtig abzufangen, brauchst du ein Exception-Handling. Ganz richtigerweise solltest du nur bestimmte Exceptions abfangen. Da es und hier nicht um sauberes Exception-Handling geht, sondern wir den Code übersichtlich halten wollten, fangen wir alle Exceptions ab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for u in tqdm(urls):\n",
    "    try:\n",
    "        filename = article_filename(u)\n",
    "        html = open(filename).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "    except:\n",
    "        # Datei nicht gefunden, invalides HTML etc.\n",
    "        continue\n",
    "    d = {}\n",
    "    d[\"title\"] = soup.h1.text.strip()\n",
    "    try:\n",
    "        d[\"header\"] = soup.select_one(\"#meldung > div.article-layout__header-container > header > p\").text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        d[\"author\"] = soup.select_one(\"a.redakteurskuerzel__link\").attrs[\"title\"]\n",
    "    except:\n",
    "        pass\n",
    "    d[\"text\"] = \"\\n\".join([p.text.strip() \n",
    "                            for p in soup.select(\"#meldung > div.article-layout__content-container > div > p\")])\n",
    "    try:\n",
    "        d[\"keywords\"] = soup.find(\"meta\", {\"name\": \"keywords\"})[\"content\"]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        ld = json.loads(soup.find(\"script\", type=\"application/ld+json\").string)\n",
    "        for k in [\"identifier\", \"url\", \"datePublished\", \"commentCount\"]:\n",
    "            d[k] = ld[0][k]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    res.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach ungefähr 10 Minuten müsste das bei dir durchgelaufen sein und du hast nun eine große Liste mit `dict`s aller Artikel zur Verfügung.\n",
    "\n",
    "Diese wandelst du am besten in einen `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "articles = pd.DataFrame(res)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei so vielen Artikeln solltest du unbedingt etwas Qualitätskontrolle betreiben. Welche Artikel haben keinen `identifier`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles[\"identifier\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibt es welche ohne `url`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles[\"url\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sind die gleichen, bei denen sind viele Felder nicht belegt und diese Dokumente kannst du ignorieren.\n",
    "\n",
    "Deshalb kannst du den `DataFrame` so modifizieren, dass nur Dokumente übrig bleiben, deren `identifier` gesetzt ist. Dazu nutzt du die Methode `dropna`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.dropna(subset=[\"identifier\"])\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie erwartet ist also nur ein Artikel verschwunden (diese Anzahl kann sich bei dir ändern, *Heise räumt auf*).\n",
    "\n",
    "Leider gibt es jetzt noch doppelte `identifier`, die kannst du mit `drop_duplicates` löschen. Anschließend kannst du den `index` des `DataFrame` auf den `identifier` setzen und in einen Integer wandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.drop_duplicates(subset=[\"identifier\"]).set_index(\"identifier\")\n",
    "articles.index = articles.index.astype(int)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind wieder ein paar Artikel verschwunden, es bleiben nur die mit einem eindeutigen `identifier` übrig.\n",
    "\n",
    "Diese verbleibenden *sauberen Artikel* packst du jetzt in eine SQLite-Datenbank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sql = sqlite3.connect(\"heise-articles-2020.db\")\n",
    "articles.to_sql(\"articles\", sql, index_label=\"id\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Artikeln können wir in den folgenden Kapitel weiterarbeiten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
