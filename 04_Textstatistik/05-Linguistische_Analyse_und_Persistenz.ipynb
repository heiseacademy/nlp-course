{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistische Analyse und Persistenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du gerade gesehen hast, dauert die Analyse eine Korpus eine ganze Weile. Daher bietet es sich an, dass du nach einer erfolgten Analyse die Ergebnisse abspeicherst, um später wieder darauf zugreifen zu können. Die Datenbank ist sehr gut geeignet dafür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell und Daten laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du beginnst mit dem Einladen des Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du die gesamten Daten für die Analyse einladen. Häufig bietet es sich an, mittels `LIMIT` im SQL die Anzahl der Datensätze zu beschränken, um die Analyse zunächst zu testen. Das kannst du dir hier sparen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM articles ORDER BY datePublished\", sql, index_col=\"id\", parse_dates=[\"datePublished\"])\n",
    "df[\"full_text\"] = df[\"title\"] + \"\\n\" + df[\"header\"] + \"\\n\" + df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die eigentliche Analyse dauert etwa zehn Minuten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# einen einzelnen Satz analysieren\n",
    "def analyze_sentence(sent):\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            nouns.append(token.lemma_ )\n",
    "            nav.append(token.lemma_ )\n",
    "        if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "            adjectives.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbs.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "        lemmas.append(token.lemma_)    \n",
    "        \n",
    "    return (nouns, adjectives, verbs, lemmas, nav, \n",
    "            [str(e) for e in sent.ents], [str(nc) for nc in sent.noun_chunks])\n",
    "\n",
    "# die einzelnen Teil der Sätze wieder miteinander verbinden\n",
    "def resentence(words):\n",
    "    # Sätze kannst du nicht mit \".\" verbinden, weil das auch ein Teil einer Entität sein kann\n",
    "    # bei \"#\" ist das deutlich unwahrscheinlicher\n",
    "    # Wörter verbindest du wie gewohnt mit \"|\"\n",
    "    # leere \"Sätze\" werden ignoriert (also solche ohne Entitäten oder Adjektive)\n",
    "    return \"#\".join([\"|\".join([w for w in sent_words])\n",
    "                                    for sent_words in words if len(sent_words) > 0])\n",
    "\n",
    "# Iteration über den gesamten Dataframe\n",
    "for i, r in tqdm(df.iterrows(), total=len(df)):\n",
    "    doc = nlp(str(r[\"full_text\"]))\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    entities = []\n",
    "    noun_chunks = []\n",
    "    for sentence in doc.sents:\n",
    "        # Satz analysieren\n",
    "        (sent_nouns, sent_adjectives, sent_verbs, sent_lemmas, sent_nav,\n",
    "         sent_entities, sent_noun_chunks) = analyze_sentence(sentence)\n",
    "            \n",
    "        # Werte für jeden Satz speichern\n",
    "        nouns.append(sent_nouns)\n",
    "        adjectives.append(sent_adjectives)\n",
    "        verbs.append(sent_verbs)\n",
    "        nav.append(sent_nav)\n",
    "        lemmas.append(sent_lemmas)\n",
    "        entities.append(sent_entities)\n",
    "        noun_chunks.append(sent_noun_chunks)\n",
    "      \n",
    "    # zusammengesetzte Sätze abspeichern\n",
    "    df.at[i, \"nouns\"]       = resentence(nouns)\n",
    "    df.at[i, \"adjectives\"]  = resentence(adjectives)\n",
    "    df.at[i, \"verbs\"]       = resentence(verbs)\n",
    "    df.at[i, \"lemmas\"]      = resentence(lemmas)\n",
    "    df.at[i, \"nav\"]         = resentence(nav)\n",
    "    df.at[i, \"entities\"]    = resentence(entities)\n",
    "    df.at[i, \"noun_chunks\"] = resentence(noun_chunks)\n",
    "\n",
    "    df.at[i, \"no_tokens\"]      = df.at[i, \"lemmas\"].count(\"|\") + 1\n",
    "    df.at[i, \"no_sentences\"]   = len(lemmas)\n",
    "    df.at[i, \"no_noun_chunks\"] = df.at[i, \"noun_chunks\"].count(\"|\") + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten abspeichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kannst du die Daten in der Datenbank persistieren, damit kannst du dann in Zukunft direkt mit den bereits analysierten Daten weiterarbeiten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"nlp_articles\", sql, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`if_exists=\"replace\"` erlaubt dir die mehrfache Ausführung des Notebooks, da eine evtl. vorhandene Tabelle einfach überschrieben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis für weitere Auswertungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der `nlp_articles`-Tabelle werden wir in Zukunft häufig arbeiten, da hier alle linguistischen Informationen bereits berücksichtigt sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
