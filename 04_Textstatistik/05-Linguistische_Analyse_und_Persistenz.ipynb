{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM articles ORDER BY datePublished\", sql, index_col=\"id\", parse_dates=[\"datePublished\"])\n",
    "df[\"full_text\"] = df[\"title\"] + \"\\n\" + df[\"header\"] + \"\\n\" + df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# einen einzelnen Satz analysieren\n",
    "def analyze_sentence(sent):\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            nouns.append(token.lemma_ )\n",
    "            nav.append(token.lemma_ )\n",
    "        if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "            adjectives.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbs.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "        lemmas.append(token.lemma_)    \n",
    "        \n",
    "    return (nouns, adjectives, verbs, lemmas, nav, \n",
    "            [str(e) for e in sent.ents], [str(nc) for nc in sent.noun_chunks])\n",
    "\n",
    "# die einzelnen Teil der Sätze wieder miteinander verbinden\n",
    "def resentence(words):\n",
    "    # Sätze kannst du nicht mit \".\" verbinden, weil das auch ein Teil einer Entität sein kann\n",
    "    # bei \"#\" ist das deutlich unwahrscheinlicher\n",
    "    # Wörter verbindest du wie gewohnt mit \"|\"\n",
    "    # leere \"Sätze\" werden ignoriert (also solche ohne Entitäten oder Adjektive)\n",
    "    return \"#\".join([\"|\".join([w for w in sent_words])\n",
    "                                    for sent_words in words if len(sent_words) > 0])\n",
    "\n",
    "# Iteration über den gesamten Dataframe\n",
    "for i, r in tqdm(df.iterrows(), total=len(df)):\n",
    "    doc = nlp(str(r[\"full_text\"]))\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "    entities = []\n",
    "    noun_chunks = []\n",
    "    for sentence in doc.sents:\n",
    "        # Satz analysieren\n",
    "        (sent_nouns, sent_adjectives, sent_verbs, sent_lemmas, sent_nav,\n",
    "         sent_entities, sent_noun_chunks) = analyze_sentence(sentence)\n",
    "            \n",
    "        # Werte für jeden Satz speichern\n",
    "        nouns.append(sent_nouns)\n",
    "        adjectives.append(sent_adjectives)\n",
    "        verbs.append(sent_verbs)\n",
    "        nav.append(sent_nav)\n",
    "        lemmas.append(sent_lemmas)\n",
    "        entities.append(sent_entities)\n",
    "        noun_chunks.append(sent_noun_chunks)\n",
    "      \n",
    "    # zusammengesetzte Sätze abspeichern\n",
    "    df.at[i, \"nouns\"]       = resentence(nouns)\n",
    "    df.at[i, \"adjectives\"]  = resentence(adjectives)\n",
    "    df.at[i, \"verbs\"]       = resentence(verbs)\n",
    "    df.at[i, \"lemmas\"]      = resentence(lemmas)\n",
    "    df.at[i, \"nav\"]         = resentence(nav)\n",
    "    df.at[i, \"entities\"]    = resentence(entities)\n",
    "    df.at[i, \"noun_chunks\"] = resentence(noun_chunks)\n",
    "\n",
    "    df.at[i, \"no_tokens\"]      = df.at[i, \"lemmas\"].count(\"|\") + 1\n",
    "    df.at[i, \"no_sentences\"]   = len(lemmas)\n",
    "    df.at[i, \"no_noun_chunks\"] = df.at[i, \"noun_chunks\"].count(\"|\") + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"nlp_articles\", sql, if_exists=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
