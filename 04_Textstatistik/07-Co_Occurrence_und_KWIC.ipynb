{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Occurrence und KWIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du im letzten Teil gesehen hast, konnten wir relativ einfach Wordclouds generieren und anzeigen. Diese zeigen dir, welche Themen im Heise Newsticker behandelt werden.\n",
    "\n",
    "Allerdings fehlen dir zur genaueren Interpretation noch *Kontextinformationen*. Die wirst du nun ermitteln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du beginnst zunächst wieder mit dem Laden der linugistisch analysierten Daten aus der Datenbank. Das wird auch in Zukunft häufig dein erster Arbeitsschritt sein. Wenn du möchtest, kannst du auch nur eine Teilmenge mit `LIMIT` auswählen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles ORDER BY datePublished\", sql, index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vokabular bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt bestimmst du nun das Vokabular, das du analysieren möchtest. Dazu benötigst du wieder die Funktion, die die Felder in die Tokens zerlegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def single_words(df, field):\n",
    "    return [w for words in df[field] for w in re.split(r'\\||\\#', words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig ist das `nav`-Feld, das Substantive, Adjektive und Verben enthält, besonders gut für die Analyse geeignet. Mithilfe eines `Counter` bestimmst du die häufigsten Wörter in dem Feld, lässt aber die Stopwords weg. Die 10.000 häufigsten Wörter speicherst du in einer separaten Variable `voc` ab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "nav = Counter(single_words(df, \"nav\"))\n",
    "for w in STOP_WORDS:\n",
    "    nav[w] = 0\n",
    "    \n",
    "voc = [w[0] for w in nav.most_common(10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem Fenster von fünf Wörtern werden nun die sog. *Co-Occurrences* ermittelt. Das sind die Wörter, die dort gemeinsam auftreten. Auch hier werden die Stopwords ausgelassen. Das Fenster wird vom \"mittleren\" Wort halb nach vorne und halb nach hinten gerechnet. Die Konstruktion mit dem `defaultdict` und dem `Counter` erspart dir viele `if`-Abfragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "# defaultdict liefert das Argument zurück, wenn key noch unbekannt\n",
    "# in diesem Fall also einen neuen (leeren) Counter\n",
    "c = defaultdict(lambda: Counter())\n",
    "\n",
    "window = 5 # sollte ungerade sein\n",
    "skip = (window - 1) // 2\n",
    "for doc in tqdm(df[\"nav\"]):\n",
    "    # Stopwords eliminieren\n",
    "    tokens = [w for w in re.split(r'\\||\\#', doc) if w not in STOP_WORDS]\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in voc:\n",
    "            for j in range(max(0, i-skip), i):\n",
    "                if tokens[j] in voc:\n",
    "                    c[w][tokens[j]] += 1\n",
    "            for j in range(i+1, min(i+1+skip, len(tokens))):\n",
    "                if tokens[j] in voc:\n",
    "                    c[w][tokens[j]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du ermitteln, welche Begriffe besonders häufig zusammen mit `Apple` vorkommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"Apple\"].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht schon sehr gut aus. Teste es noch mit `Google`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"Google\"].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch dieses Ergebnis ist richtig gut und wie erwartet. Die Google-Dienste erscheinen eigentlich alle und du kannst damit feststellen, dass die Meldungen nicht nur die richtigen Wörter enthalten, sondern auch die richtigen Themen besprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt weißt du aber noch nicht, wie die Wörter wirklich verwendet werden. Dazu kann Keyword-in-Context [KWIC](https://de.wikipedia.org/wiki/Permutiertes_Register) helfen. Dazu werden für die Einzelmeldungen jeweils immer ein bestimmtes Festner von Buchstaben angezeigt, die um das gewählte Wort vorkommen. Die Funktion ist einfach implementiert, ein *regulärer Ausdruck* wird dazu dynamisch konstruiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def kwic(word, texts, window_size):\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        context = (window_size//2)*'.'\n",
    "        kwic = context + r'\\b' + word + r'\\b' + context\n",
    "        match = re.findall(kwic, text)\n",
    "        for m in match:\n",
    "            res.append(m)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion erfordert ein Array von Strings, das kannst du aus dem `DataFrame` einfach ermitteln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"text\"].map(str).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Aufruf kann einen Augenblick dauern, weil es viele Meldungen mit `Apple` gibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic(\"Apple\", text, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kannst du die Einzelmeldungen sehen. Alle zu lesen, ist natürlich etwas aufwändiger, als die Co-Occurrence oben auszuwerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Context is King*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Texte genauer zu verstehen, sind Kontextinformationen eminent wichtig. Mit *Co-Occurrence* und *KWIC* hast du zwei Methoden kennengelernt, mit denen du den Kontext einfach bestimmen kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
