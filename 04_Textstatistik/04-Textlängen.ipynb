{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bestimmung von Textlängen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du viele Dokumente analysieren möchtest, spielt deren individuelle Länge eine große Rolle. Das gilt nicht nur für die Größe in Bytes, sondern auch die Anzahl der Zeichen (das ist *nicht* identisch z.B. wegen der UTF-8-Kodierung), die Anzahl der Tokens, der Sätze usw.\n",
    "\n",
    "`textacy` und `spacy` bieten hier eine gute Kombination von Auswertungsmöglichkeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle Dokumente verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für dieses Beispiel wirst du nun alle Dokumente analysieren lassen, dazu wählst du alle aus der Datenbank aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM articles ORDER BY datePublished\", sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da dich der gesamte Text interessiert, fügst du die einzelnen Textfelder zusammen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_text\"] = df[\"title\"] + \"\\n\" + df[\"header\"] + \"\\n\" + df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Textanalyse benötigt `textacy` die Hilfe von `spacy`, das du als Modul importierst und das Modell lädst (diesmal aber nur mit dem `parser`, damit die Analyse etwas schneller funktioniert):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\", disable=(\"parser\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analyse dauert dennoch ein paar Minuten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "corpus = textacy.Corpus(nlp, data = list(df[\"full_text\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kannst du auf statistische Informationen des gesamtem Corpus komfortabel zugreifen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der Tokens entspricht der Länge des Dokuments und du kannst eine eigene Spalte im `DataFrame` belegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [len(d) for d in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der Buchstaben kannst du auch leicht ermitteln (allerdings sind darin Leerzeichen und Umbrüche auch enthalten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"characters\"] = df[\"full_text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `describe` kannst du dir statistiscge Informationen zu den Spalten ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daran erkennst du schon einiges an der Datenstruktur, allerdings kannst du das besser visuell erkunden. Dazu verwendest du *Histpgramme*, die dir die Anzahl der Dokumente in einzelnen Wertebereichen anzeigen. Betrachte zuerst die Anzahl der Tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"tokens\"]].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du eine genauere Auflösung brauchst, kann du mehr sog. *Bins* verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"tokens\"]].plot.hist(bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie in der *Five-Number-Summary* oben kannst du hier gut erkennen, dass die meisten Artikel zwischen 300 und 500 Tokens haben. Es gibt wenige, die deutlich länger sind.\n",
    "\n",
    "Du kannst dir auch die Anzahl der Buchstaben als Histogramm ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"characters\"]].plot.hist(bins=40, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch damit ergibt sich die typische Kurve, die du bereits bei der Anzahl der Tokens gesehen hast. Selbstverständlich kannst du die Auswertungen miteinander kombinieren, um z.B. die typische Länge der Tokens in Zeichen zu ermitteln usw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textlängen nicht unterschätzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den nächsten Auswertungen wirst du dich noch häufiger mit Textlängen auseinandersetzen. Besonders im Zeitverlauf ist es neben der Anzahl der Texte ein entscheidendes Kriterium, wie aussagekräftig ein Korpus ist. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
