{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textstatistik: `textacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben `spacy` wirst du häufig auch `textacy` zur Vorverarbeitung von Texten einsetzen. `textacy` hat sehr viele ganz unterschiedliche Funktionen, die am Anfang etwas zu überblicken sind.\n",
    "\n",
    "In diesem Teil wirst du daher einige der Möglichkeiten kennenlernen, für die man `textacy` besonders gut einsetzen kann.\n",
    "\n",
    "Achtung: Zum aktuellen Zeitpunk (März 2021) funktioniert `textacy` nur eingeschränkt gut mit `spacy` in der Version 3.0 zusammen. Gemeinsam mit dem Autor von `textacy` arbeiten wir an einer Lösung. Sollte es uns nicht gelingen, dauerhaft eine Kompatibilität herzustellen, werden wir ein abgeleitetes Textacy-Paket mit in das Github-Archiv legen. Dann solltest du `textacy` nicht separat installieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Aufgabe, die du im letzten Kapitel schon selbstständig gelöst hast, kann `textacy` auch erledigen: das Wiederzusammenfügen von Trennungen. Betrachte dazu nochmals den Abschnitt aus dem PDF-Dokument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Figure 7:  Schematic inheritance relations and properties for the top-level Self-Description  \n",
    "Schemas.\n",
    "\n",
    "© BMWi\n",
    "\n",
    "132  CORE ARCHITECTURE ELEMENTS\n",
    "\n",
    "2.5 Catalogue\n",
    "\n",
    "The concept Self-Description is the foundation of the \n",
    "federated GAIA-X Catalogues. Catalogues are the \n",
    "main building block for the publication and discovery \n",
    "of Self-Descriptions of Assets and Participants. To sat-\n",
    "isfy Consumer needs and to objectively find the best \n",
    "fitting offerings in the tangle of registered Assets, an \n",
    "open and transparent query algorithm is implemented \n",
    "without any GAIA-X internal ranking. Beside search \n",
    "functionality, a graph-based navigation interface is \n",
    "provided to traverse the complex tangle of offered \n",
    "Services, Nodes and linked Self-Descriptions, includ-\n",
    "ing the attached claims with chain of trust statements. \n",
    "Consumers can verify each Self-Description individu-\n",
    "ally and decide which one to select in a self-sovereign \n",
    "manner – GAIA-X does not act as a runtime interme-\n",
    "diary or broker.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe des `preprocessing`-Moduls von `textacy` kannst du die Trennungen direkt zusammenführen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import textacy.preprocessing\n",
    "text_hyphen = textacy.preprocessing.normalize.hyphenated_words(text)\n",
    "print(text_hyphen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis sieht zum Glück genau aus wie mit unserer eigenen Methode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` kann noch weitere Normalisierungen durchführen, wie die Entfernung von mehrfachen *Whitespaces*. Nachdem Leerzeilen auch als Whitespaces betrachtet werden, führt das zu einem Text ohne Leerzeilen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textacy.preprocessing.normalize.whitespace(text_hyphen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier siehst du, dass durch die Vorverarbeitung tatsächlich Informationen verloren gegangen sind. So ist nun nicht mehr ersichtlich, wo wirklich ein neuer Absatz beginnt. Deshalb solltest du mit der Filterung auch vorsichtig umgehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basisdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für einige weitere Funktionen von `textacy` betrachten wir diesmal nur dern ersten Paragraph aus dem Neujahrs-Artikel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Doch das Ende des Jahres 2020 birgt auch Hoffnung, dass durch die Vakzinen \\\n",
    "gegen Covid-19 wieder Normalität einkehre – wie immer die auch aussehen mag \\\n",
    "– und wir uns um anderes Dringliches kümmern oder einfach entspannen \\\n",
    "können. Und dass durch den im Januar anstehenden Bewohnerwechsel im \\\n",
    "Weißen Haus zu Washington D.C. das offizielle Herumgetrumpel auf dem \\\n",
    "gesunden Menschenverstand ein Ende finden möge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` hat eine Funktion `KWIC`, was für *Keywords in Context* steht und kann dir damit anzeigen, in welchem Kontext ein Wort vorkommt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.kwic.keyword_in_context(p1, \"Hoffnung\", window_width=35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion ist auch ganz einfach selbst implementiert (wie du später noch genauer sehen wirst), kann aber sehr nützlich sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Gramme und Noun Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texacy` kann sehr eng mit `spacy` zusammenarbeiten und auch die `spacy`-Dokumente übernehmen. Damit kannst du dir schnell interessante Statistiken erschließen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = textacy.load_spacy_lang(\"de_core_news_lg\")\n",
    "doc = textacy.make_spacy_doc(p1, lang=de)\n",
    "doc._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy`macht es dir sehr einfach, Wortkombinationen (sog. *n-Gramme*) zu ermitteln und dabei gleich bestimmte Tokentypen von `spacy` auszufiltern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.ngrams(doc, 3, filter_stops=True, filter_punct=True, \n",
    "                            filter_nums=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die aus `spacy` bekannten Entitäten kannst du dir schnell ermitteln lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.entities(doc, drop_determiners=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` kann sog. *Noun Phrases* ermitteln. Im Gegensatz zu früheren Versionen, die dabei auf *Regular Expressions* aufgesetzt hat, wird jetzt der *Dependency Parser* von `spacy` dafür verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.noun_chunks(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank und Worte zählen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der TextRank-Algorithmus versucht, besonders wichtige Konstrukte zu bewerten und diese entsprechend zu sortieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.extract.keyterms\n",
    "textacy.extract.keyterms.textrank(doc, normalize=\"lemma\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mihilfe der Methode `to_bag_of_term` kannst du die Häufigkeit der Wörter zählen. Der `Counter` von Python bietet dir dann komfortable Zugriffsmöglichkeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bot = Counter(doc._.to_bag_of_terms(ngs=(1, 2, 3), ents=True, \n",
    "                                    weighting=\"count\")) #, as_strings=True))\n",
    "bot.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehrere Dokumente verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was mit einem Dokument funktioniert, kannst du natürlich auch für mehrere Dokumente des gesamten Corpus durchführen. Dazu wählst du zunächst 100 Dokumente aus der Datenbank aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 Dokumente selektieren\n",
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM articles \\\n",
    "                  ORDER BY datePublished DESC LIMIT 100\", sql)\n",
    "df[\"full_text\"] = df[\"title\"] + \"\\n\" + df[\"header\"] + \"\\n\" + df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` hat eine eigene Datenstruktur für einen `Corpus`, die du mit den Dokumenten initalisieren kannst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "corpus = textacy.Corpus(\"de_core_news_lg\", \n",
    "                        data = list(df[\"full_text\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit stehen dir gleich Statistik-Informationen zur Verfügung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die Worthäufigkeiten selbst kannst du von `textacy` zählen lassen und packst sie danach am besten wieder in einen `Counter`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(corpus.word_counts(by=\"lemma_\"))\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noch viel mehr Möglichkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` kann noch deutlich mehr. Allerdings werden fast alle weitergehenden Funktionen von `textacy` auch in anderen Notebooks vorgestellt. Dennoch kannst du `textacy` gut als *Schweizer Taschenmesser* nutzen und viele Funktionen darin finden.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensets zum Ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.datasets\n",
    "ds = textacy.datasets.CapitolWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.download()\n",
    "records = ds.records(speaker_name={\"Hillary Clinton\", \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
