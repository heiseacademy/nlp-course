{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Teil wirst du kennenlernen, wie du mithilfe einer sog. [Support Vector Machine](https://de.wikipedia.org/wiki/Support_Vector_Machine) (SVM) Daten sehr einfach klassifizieren kannst. Die SVM ist nur eine Möglichkeit der Klassifikation, es gibt auch noch weiter Modelle, die du später kennenlernen wirst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle diese Felder kommen damit prinzipiell als Kategorien in Frage. Betrachte sie nun nacheinander:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `author`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du benögtigst mindestens ein paar Hundert Datensätze, um den Klassifikator zu trainieren. Wenn du für Artikel die Autoren vorhersagen möchtest, müsstest du daher Autoren finden, die mindestens 100 Artikel geschrieben haben. Betrachte dazu die Top-20-Autoren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = df.groupby(\"author\").count().sort_values(\"title\", ascending=False).head(20)[[\"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit du zu einer ausgeglichenen Trainingsmenge kommst, suchst du den Top-Autor mit den wenigsten Artikeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_articles = min(top_authors[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun konstruierst du einen `DataFrame`, der von allen Autoren gleich viele Artikel enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.concat([df[df[\"author\"] == author].sample(min_articles, random_state=42)\n",
    "                     for author in top_authors.index.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du die Daten vektorisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(adf[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell lässt sich sehr schnell trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(tfidf_vectors, adf[\"author\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Vorhersage kannst du ebenso einfach durchführen und die Ergebnisse im `DataFrame` abspeichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf[\"predicted_author\"] = svc.predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließlich kannst du zählen, wie häufig das Modell richtig und falsch klassifiziert hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(adf[adf[\"author\"] == adf[\"predicted_author\"]]))\n",
    "print(len(adf[adf[\"author\"] != adf[\"predicted_author\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das hat *außerordentlich* gut funktioniert. Das Modell ist so gut trainiert, dass es nur in 13 von 2.600 Fällen eine falsche Vorhersage liefert, das sind nur 0,5% falsche Werte. Allerdings könnte es sein, dass es die Daten nur auswendig gelernt hat. Das werden wir später untersuchen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `keywords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die Keywords könnten eine gute Wahl für eine bereits vorklassifizierte Kategorie sein. Allerdings sind pro Meldung mehrere Keywords vergeben. Wenn du die zählen möchtest, machst du das am besten mit einem `Counter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keywords = Counter([keyword for keywords in df[\"keywords\"] for keyword in str(keywords).split(\", \")])\n",
    "top_keywords = [keyword[0] for keyword in keywords.most_common(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun musst du allerdings anders vorgehen. Für jedes Keyword trainierst du ein eigenes Modell, indem du jeweils einen `DataFrame` konstruierst, der in gleicher Zahl positive und negative Beispiele für das Keywords enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in top_keywords:\n",
    "    # DataFrame mit/ohne Keyword bestimmen\n",
    "    k_pos = df[df[\"keywords\"].map(str).str.contains(keyword)].copy()\n",
    "    k_pos[\"keyword\"] = 1\n",
    "    k_neg = df[~ df[\"keywords\"].map(str).str.contains(keyword)].copy()\n",
    "    k_neg[\"keyword\"] = 0\n",
    "    \n",
    "    # kleinste Länge ausrechnen\n",
    "    min_keyword = min(len(k_pos), len(k_neg))\n",
    "    kdf = pd.concat([k_pos.sample(min_keyword, random_state=42),\n",
    "                     k_neg.sample(min_keyword, random_state=42)])\n",
    "    \n",
    "    # vektorisiere\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(kdf[\"nav\"])\n",
    "    \n",
    "    # trainieren\n",
    "    svc = SVC()\n",
    "    svc.fit(tfidf_vectors, kdf[\"keyword\"])\n",
    "    \n",
    "    # vorhersagen\n",
    "    kdf[\"predicted_keyword\"] = svc.predict(tfidf_vectors)\n",
    "    \n",
    "    # richtig/falsch berechnen\n",
    "    print(keyword)\n",
    "    print(len(kdf[kdf[\"keyword\"] == kdf[\"predicted_keyword\"]]))\n",
    "    print(len(kdf[kdf[\"keyword\"] != kdf[\"predicted_keyword\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch diese Klassifikation hat sehr gut funktioniert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `commentCount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst normalisierst du die Kommentare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"] = df[\"commentCount\"].fillna(0).map(int)\n",
    "df.loc[df[\"normalizedCommentCount\"]>500, \"normalizedCommentCount\"] = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann konstruierst du zwei `DataFrame`, in denen erfolgreich und nicht erfolgreiche Posts enthalten sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"normalizedCommentCount\"]>50].copy()\n",
    "df_success[\"success\"] = 1\n",
    "\n",
    "df_no_success = df[df[\"normalizedCommentCount\"]<10].copy()\n",
    "df_no_success[\"success\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du berechnest die Größe des kleineren `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_success = min(len(df_success), len(df_no_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und erzeugst ein ausgeglichenes Trainingsset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.concat([df_success.sample(min_success, random_state=42),\n",
    "                 df_no_success.sample(min_success, random_state=42)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten vektorisierst du nun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sdf[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainierst das Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(tfidf_vectors, sdf[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du führst nun die Vorhersage durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[\"predicted_success\"] = svc.predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und bewertest das Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sdf[sdf[\"success\"] == sdf[\"predicted_success\"]]))\n",
    "print(len(sdf[sdf[\"success\"] != sdf[\"predicted_success\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch das hat extrem gut funktioniert. Es wäre super, wenn man aus dem Artikel schon vorher prädizieren könnte, wie erfolgreich der Artikel wird. Die Heise-Redakteure können das wahrscheinlich intuitiv!\n",
    "\n",
    "Wir werden später allerdings überprüfen müssen, ob der Klassifikator die Daten nicht nur auswendig gelernt hat! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifikation ist nicht schwierig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du gesehen hast, kann man einen Klassifikator schnell trainieren und in dem Datenset funktionieren auch alle Klassifikatoren wirklich sehr gut.\n",
    "\n",
    "Später überprüfst du, wie gut der Algorithmus mit unbekannten Daten umgehen (also *abstrahieren*) kann und du wirst Gütekriterien kennenlernen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
