{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = df.groupby(\"author\").count().sort_values(\"title\", ascending=False).head(20)[[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_articles = min(top_authors[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.concat([df[df[\"author\"] == author].sample(min_articles, random_state=42)\n",
    "                     for author in top_authors.index.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(adf[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(tfidf_vectors, adf[\"author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf[\"predicted_author\"] = svc.predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(adf[adf[\"author\"] == adf[\"predicted_author\"]]))\n",
    "print(len(adf[adf[\"author\"] != adf[\"predicted_author\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keywords = Counter([keyword for keywords in df[\"keywords\"] for keyword in str(keywords).split(\", \")])\n",
    "top_keywords = [keyword[0] for keyword in keywords.most_common(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in top_keywords:\n",
    "    # DataFrame mit/ohne Keyword bestimmen\n",
    "    k_pos = df[df[\"keywords\"].map(str).str.contains(keyword)].copy()\n",
    "    k_pos[\"keyword\"] = 1\n",
    "    k_neg = df[~ df[\"keywords\"].map(str).str.contains(keyword)].copy()\n",
    "    k_neg[\"keyword\"] = 0\n",
    "    \n",
    "    # kleinste LÃ¤nge ausrechnen\n",
    "    min_keyword = min(len(k_pos), len(k_neg))\n",
    "    kdf = pd.concat([k_pos.sample(min_keyword, random_state=42),\n",
    "                     k_neg.sample(min_keyword, random_state=42)])\n",
    "    \n",
    "    # vektorisiere\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(kdf[\"nav\"])\n",
    "    \n",
    "    # trainieren\n",
    "    svc = SVC()\n",
    "    svc.fit(tfidf_vectors, kdf[\"keyword\"])\n",
    "    \n",
    "    # vorhersagen\n",
    "    kdf[\"predicted_keyword\"] = svc.predict(tfidf_vectors)\n",
    "    \n",
    "    # richtig/falsch berechnen\n",
    "    print(keyword)\n",
    "    print(len(kdf[kdf[\"keyword\"] == kdf[\"predicted_keyword\"]]))\n",
    "    print(len(kdf[kdf[\"keyword\"] != kdf[\"predicted_keyword\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"] = df[\"commentCount\"].fillna(0).map(int)\n",
    "df.loc[df[\"normalizedCommentCount\"]>500, \"normalizedCommentCount\"] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"normalizedCommentCount\"]>50].copy()\n",
    "df_success[\"success\"] = 1\n",
    "\n",
    "df_no_success = df[df[\"normalizedCommentCount\"]<10].copy()\n",
    "df_no_success[\"success\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_success = min(len(df_success), len(df_no_success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.concat([df_success.sample(min_success, random_state=42),\n",
    "                 df_no_success.sample(min_success, random_state=42)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sdf[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(tfidf_vectors, sdf[\"success\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[\"predicted_success\"] = svc.predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sdf[sdf[\"success\"] == sdf[\"predicted_success\"]]))\n",
    "print(len(sdf[sdf[\"success\"] != sdf[\"predicted_success\"]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
