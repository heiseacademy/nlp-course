{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall und Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Teil beschäftigst du dich damit, wie du die Abstraktionsfähigkeit eines Modells und damit seine Vorhersagequalität mit Gütekriterien messen kannst.\n",
    "\n",
    "Du kennst schon die Confusion Matrix, damit verstehst du im Detail, wie ein Klassifikator sich verhält. Oft möchtest du das aber auf eine (oder mehrere) Kenngröße/n reduzieren, die du in diesem Teil kennenlernen wirst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Teil betrachtest du nur die Klassifikation nach Autoren. Genauso kannst du das natürlich für Keywords (einzeln!) oder für die Kommentare durchführen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten für Autoren-Klassifikation vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du bestimmst zunächst die Top-Autoren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = df.groupby(\"author\").count().sort_values(\"title\", ascending=False).head(20)[[\"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Autoren sollen gleich viele Artikel erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_articles = min(top_authors[\"title\"])\n",
    "adf = pd.concat([df[df[\"author\"] == author].sample(min_articles, random_state=42)\n",
    "                     for author in top_authors.index.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du die Daten vektorisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(adf[\"nav\"])\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch zur Berechnung der Qualitätsmetriken benötigst du einen Split in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(tfidf_vectors, adf[\"author\"].values, \n",
    "                                                      train_size=0.75, random_state=42,\n",
    "                                                      stratify=adf[\"author\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Auswertung SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Training kennst du schon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svc = svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lass dir nochmal ausrechnen, wie oft der Klassifikator richtig oder falsch liegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_test  = svc.predict(X_test)\n",
    "np.unique(pred_test == y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig wird in Klassifikationsaufgaben (oder allemeiner beim Information Retrieval) die sog. Accuracy als Kenngröße angegeben. Dabei handelt es sich um den Anteil der richtig klassifizierten Ergebnisse am Gesamtanteil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben der Accuracy gibt es aber noch weitere Metriken, die besonders in *schiefen* Datensets eine Rolle spielen. Als Beispiel betrachten wir einen Test für eine sehr seltene Krankheit. Wenn du diesen so \"implementierst\", dass er immer *negativ* zurückgibt, würdest du eine sehr hohe Accuracy erhalten. Dabei funktioniert der Test gar nicht.\n",
    "\n",
    "Dafür gibt es andere Maßzahlen, nämlich [Precision und Recall](https://de.wikipedia.org/wiki/Beurteilung_eines_bin%C3%A4ren_Klassifikators#Anwendung_im_Information_Retrieval). Precision (oder auch Spezifizität) gibt dabei an, wie viele der gefundenen Ergebnisse richtig klassifiziert wurden. Recall sagt dir, wie viele der gewünschten Ergebnisse gefunden werden konnten. Leider arbeiten die beiden etwas gegeneinander.\n",
    "\n",
    "Betrachte dazu das Beispiel und lass dir einen Report ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, wurden z.B. die Artikel von `Alexander Neumann` fast alle richtig klassifiziert (`94%`), allerdings wurden nur `50%` gefunden. Schlechter sieht es z.B. bei `Andreas Wilkens` aus und ganz schlecht (du ahnst es) bei `Tilman Wittenhorst`. Von ihm wurden zwar immerhin `41%` der Artikel gefunden, aber nur `11%` der ihm zugeordneten Artikel waren überhaut von ihm (das hast du in der hellen Spalte der Confusion Matrix gesehen).\n",
    "\n",
    "Bei anderen Autoren wie `Isabel Grünwald` und `Mark Mantel` hat das sehr viel besser funktioniert. Vermutlich liegt das daran, dass sie  eine deutlichere Themenausrichtung haben und daher einige Keywords erkannt wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich der Klassifikatoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesem Handwerkszeug ausgestattet kannst du nun überprüfen, wie sich die anderen Klassifikatoren verhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "for clf_class in [SVC, SGDClassifier, MultinomialNB, DecisionTreeClassifier,\n",
    "            RandomForestClassifier, GradientBoostingClassifier]:\n",
    "    clf = clf_class(random_state=42) if clf_class != MultinomialNB else clf_class()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_test  = clf.predict(X_test)\n",
    "    print(clf_class.__name__)\n",
    "    print(accuracy_score(y_test, pred_test), \n",
    "          precision_score(y_test, pred_test, average='weighted'), \n",
    "          recall_score(y_test, pred_test, average='weighted'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die identischen Werte für `accuracy` und den gewichteten `recall` sind auf den ersten Blick erstaunlich - das liegt aber an der Konstruktion unseres \"ausgeglichenen\" Trainingssets.\n",
    "\n",
    "Der `SGDClassifier` funktioniert offenbar am besten. Du kannst dir nochmal genau ausgeben lassen, wie die Performance zu bewerten ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "pred_test  = sgd.predict(X_test)\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, ist der `SGDClassifier` besonders im `recall` deutlich besser als `SVC`.  Abhängig von deinem Anwendungsfall kann es besser sein, entweder Precision oder Recall zu optimieren.\n",
    "\n",
    "Wenn du möchtest, versuche auch den `TfidfVectorizer` zu optimieren, indem du z.B. noch Bigramme zulässt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gütekriterien sind unbedingt notwendig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Klassifikationsmodell (oder auch ein Regressionsmodell) ohne Gütekriterien ist nur wenig wert. Leider wirst du in der Praxis häufig sehen, dass das einfach vernachlässigt wird. Solchen Modellen solltest du wenig Glauben schenken.\n",
    "\n",
    "Du solltest immer darauf achten, in deiner eigenen Arbeit mit diesen Qualitätskriterien zu arbeiten und sie zu berechnen. Es ist besser, du weißt, dass ein Modell *nicht funktioniert* als deine Kollegen damit arbeiten zu lassen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
