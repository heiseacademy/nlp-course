{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainingsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um überwachtes Lernen durchführen zu können, benötigst du eine sog. Trainingsmenge. Anhand dieser Menge von bereits klassifizierten Daten kann das Modell lernen, wie es unbekannte Daten klassifizieren kann.\n",
    "\n",
    "Ob das funktioniert, hängt stark von der Güte der Trainingsmenge ab. Die Trainingsmenge sollte sauber und *nachvollziehbar* klassifiziert sein, dabei aber auch besonders *ausgewogen*, d.h. die unterschiedlichen Ergebnisse sollten in ungefähr der gleichen Anzahl vorkommen.\n",
    "\n",
    "Damit das gelingt, kannst du auf bereits *explizit vorklassifizierte* Daten zurückgreifen oder auch mit *implizit klassifiziserten* Daten arbeiten. Schließlich kannst du Daten auch *manuell klassifizieren*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategorien identifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dir den Aufwand für die (fehleranfällige) manuelle Klassifikation zu ersparen, kannst du zunächst in den Daten nach geeigneten Feldern ausschau halten, die als Kategorien dienen könnten. Betrachte dazu zunächst alle Spalten des `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felder wie `title`, `header` und die gesamten linguistisch analysierten Daten enthalten ausschließlich Wörter und sind daher als Kategorien ungeeignet.\n",
    "\n",
    "Anders sieht es mit den Feldern `author`, `keywords` und `commentCount` aus. Diese enthalten kategorische (teilweise mehrwertige) oder numerische Werte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"title\", \"author\", \"keywords\", \"commentCount\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle diese Felder kommen damit prinzipiell als Kategorien in Frage. Betrachte sie nun nacheinander:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `author`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du benögtigst mindestens ein paar Hundert Datensätze, um den Klassifikator zu trainieren. Wenn du für Artikel die Autoren vorhersagen möchtest, müsstest du daher Autoren finden, die mindestens 100 Artikel geschrieben haben. Betrachte dazu die Top-20-Autoren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"author\").count().sort_values(\"title\", ascending=False).head(20)[[\"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies Autoren scheinen wirklich gut als Kategorien geeignet. Es handelt sich dabei um ein *Multi-Class Single-Label*-Problem, denn es gibt mehrere mögliche Autoren, aber jeder Artikel wurde nur von einem einzigen Autor geschrieben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `keywords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die Keywords könnten eine gute Wahl für eine bereits vorklassifizierte Kategorie sein. Allerdings sind pro Meldung mehrere Keywords vergeben. Wenn du die zählen möchtest, machst du das am besten mit einem `Counter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keywords = Counter([keyword for keywords in df[\"keywords\"] for keyword in str(keywords).split(\", \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachte nun die Top-20-Keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch diese wurden häufig genug verwendet, so dass du dafür Beispiele finden kannst. In diesem Fall hast du ein *Multi Label*-Problem, weil für jeden Artikel mehrere Keywords vergeben sein könnten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kandidat: `commentCount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die genaue Anzahl von Kommentaren zu prädizieren, wäre eine sog. *Regression*. Das wird sicher nicht klappen, weil das von zu vielen Einflussfaktoren abhängt. Aber du kannst versuchen, vorherzusagen ob ein Artikel viele oder wenige Kommentare auf sich zieht.\n",
    "\n",
    "Betrachte dazu zunächst die Verteilung der Kommentare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"commentCount\"].dropna().map(int).plot.hist(bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider ist die Verteilung sehr verzerrt, weil es wenige Artikel mit sehr vielen Kommentaren gibt. Um das zu lösen, kannst du mit einem sog. *Cutoff* arbeiten und  die Anzahl der Kommentare in besonders erfolgreichen Artikeln auf 500 begrenzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"] = df[\"commentCount\"].fillna(0).map(int)\n",
    "df.loc[df[\"normalizedCommentCount\"]>500, \"normalizedCommentCount\"] = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilung sieht damit deutlich *ausgewogener* aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"].plot.hist(bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt könntest du *willkürlich* definieren, dass Artikel mit weniger als 10 Kommentaren in eine Klasse fallen sollen und solch mit mehr als 50 in eine andere. Achtung, in solchen Fällen solltest du einen *Sicherheitsabstand* einhalten, sonst überforderst du dein Modell auf jeden Fall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"normalizedCommentCount\"]<10]))\n",
    "print(len(df[df[\"normalizedCommentCount\"]>50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du klassifizieren möchtest, ob ein Artikel erfolgreich ist oder nicht, ist das ein sog. *Single Class*-Problem - es gibt als Ergebnis nur ja oder nein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Möglichst existierende Klassen nutzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten manuell zu klassifizieren (*zu labeln*) ist sehr mühsam und extrem fehleranfällig. Crowdworker können dabei helfen.\n",
    "\n",
    "Viel geschickter ist es allerdings, vorhandene Klassen zu nutzen. Diese müssen nicht immer explizit in den Daten enthalten sein, sondern evtl. kannst du auch implizite Klassen nutzen, wie z.B. die Anzahl der Kommentare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
