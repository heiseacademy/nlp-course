{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berechnung der BERT-Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt mit den Sprachmodellen wirst du nun die Embeddings selbst ausrechnen.\n",
    "\n",
    "Du lernst dabei schon kennen, wie du ein Modell benutzt und ein bisschen auch den Umgang mit `PyTorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die Daten ein. Die Sprachmodelle nutzen einen eingebauten Tokenisierer, der auch Teilworte erkennt. Daher nützen dir die linguistisch voranalysierten Daten nicht sehr viel und du kannst einfach die Ursprungsdaten verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT id, url, title, datePublished FROM articles \\\n",
    "                  WHERE datePublished<'2021-01-01' \\\n",
    "                  ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Sprachmodelle bauen eigentlich alle auf `PyTorch` auf. `PyTorch` ist eine Bibliothek für tiefe neuronale Netze und funktioniert am besten mit einer Grafikkarten (die in diesem Notebook noch nicht unbedingt erforderlich ist):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es stehen verschiedene Sprachmodelle bereit, `bert-base-multilingual-uncased` arbeitet nur mit Kleinbuchstaben uns ist sprachunabhängig. Zunächst erzeugst du einen `BertTokenizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du auch das dazu korrespondierende Modell instanziieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion dient dazu, Texte mit BERT zu tokeniseren und anschließend deren Embeddings auszurechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    # Text mit Start- und Endmarkern ergänzen\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "    # Nur die ersten 512 Tokens verwenden\n",
    "    tokenized = tokenizer.tokenize(marked_text)[0:512]\n",
    "    # Und in IDs wandeln\n",
    "    indexed = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    # Der Einfachheit halber packst du alles in einen \"Satz\"\n",
    "    segments_ids = [1] * len(tokenized)\n",
    "    # Bisher hast du mit Listen gearbeitet, diese wandelst du jetzt in Tensoren\n",
    "    tokens_tensor = torch.tensor([indexed])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    # Nun überträgst du die Token- und Segment-Tensoren in das Modell\n",
    "    with torch.no_grad():\n",
    "        # als Ergebnis erhältst du alle (!) Embedding-Layer\n",
    "        el, _ = model(tokens_tensor, segments_tensors)\n",
    "    return el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jeden einzelnen Titel kannst du nun die Embedding-Layer ausrechnen. Den kontextualisierten Embedding-Vektor findest du im letzten Layer und mittelst diesen. Später wirst du nur den Embedding-Vektor des ersten, *vollkontextualisierten* Tokens verwenden (nämlich von `[CLS]`). Da die Berechnung ein bisschen dauert, nutzt du `tqdm` zur Forschrittsanzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tokenized_texts = []\n",
    "\n",
    "sentence_embeddings = []\n",
    "\n",
    "for text in tqdm(df[\"title\"], total=len(df)):\n",
    "    el = embed_text(text)\n",
    "    token_vecs = el[11][0]\n",
    "    embedding = torch.mean(token_vecs, dim=0)\n",
    "    sentence_embeddings.append(embedding.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Embeddings packst du jetzt in einen `DataFrame` und speicherst sie in der Datenbank ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bert = pd.DataFrame(sentence_embeddings)\n",
    "text_bert.index = df.index\n",
    "\n",
    "text_bert.to_sql(\"bert_articles\", sql, index_label=\"id\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst nun die Embedding-Layer für einen neuen Text ausrechnen (den du erfunden hast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = embed_text(\"Microsoft schafft Umsatzsprung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genau wie oben bestimmst du den Mittelwert der Embeddings aller Tokens des letzten Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = et[11][0]\n",
    "embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Cosinus-Ähnlichkeit von `scikit-learn` kannst du die (kontextualisierten) Ähnlichkeiten zu allen anderen Dokumenten berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "r = cosine_similarity(sentence_embeddings, [embedding.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `argmax` findest du den Index des Dokuments, der diesem am ähnlichsten ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[990]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings bilden kontextualisierte Ähnlichkeiten ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast gesehen, wie sich mithilfe von Sprachmodellen kontextualisierte Embeddings berechnen lassen. Der Prozess ist etwas komplizierter als bei den \"einfachen\" Word Embeddings und funktioniert grundsätzlich nur mit Sätzen oder längeren Entitäten.\n",
    "\n",
    "Kontextualisierte Ähnlichkeiten zu finden funktioniert allerdings nicht völlig überzeugend, das hat mit der semantischen Transformation viel besser geklappt. Darauf sind die BERT-Modelle auch nicht abgestimmt, sondern eher auf das sog. *Finetuning*, das du anschließend kennenlernen wirst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
