{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast nun bereits mit den BERT-Embeddings gearbeitet. Die eigentliche Leistungsfähigkeit der Sprachmodelle steckt allerdings in dem sog. *Finetuning*, d.h. sie speziell auf deine Anforderungen anzupassen.\n",
    "\n",
    "In diese Teil wirst du kennenlernen, wie sich das auf die Klassifikation der Heise-Artikel nach solchen mit vielen oder wenigen Kommentaren anwenden lässt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT enthält selbst einen Tokenisierer, daher verwendest du direkt die Titel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT id, datePublished, title, commentCount FROM articles \\\n",
    "                    WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten für Klassifikation vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst normalisierst du die Kommentare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"] = df[\"commentCount\"].fillna(0).map(int)\n",
    "df.loc[df[\"normalizedCommentCount\"]>500, \"normalizedCommentCount\"] = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann konstruierst du zwei `DataFrame`, in denen erfolgreich und nicht erfolgreiche Posts enthalten sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"normalizedCommentCount\"]>50].copy()\n",
    "df_success[\"success\"] = 1\n",
    "\n",
    "df_no_success = df[df[\"normalizedCommentCount\"]<10].copy()\n",
    "df_no_success[\"success\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du berechnest die Größe des kleineren `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_success = min(len(df_success), len(df_no_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und erzeugst ein ausgeglichenes Trainingsset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.concat([df_success.sample(min_success, random_state=42),\n",
    "                 df_no_success.sample(min_success, random_state=42)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für diesen Teil musst du sehr umfangreiche Berchnungen durchführen. Diese funktionieren zwar grundsätzlich auch auf einer CPU, allerdings würde das viele Stunden dauern. \n",
    "\n",
    "Wenn möglichst, solltest du den Code daher auf einer Grafikkarte laufen lassen, auf der das viele Größenordnungen schneller funktioniert. Solltest du auf keine Grafikkarte zugreifen können, lohnt es sich, dieses Noteboook in Google Colab auszuführen und eine entsprechende Umgebung auszuwählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU %s\" % torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU :-(\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `GeForce RTX 2070` ist zwar nicht ganz modern, aber für unsere Zwecke ist sie absolut ausreichend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der `transformers`-Bibliothen nutzt du zunächst den `BertTokenizer`, um die Titel in ihre Bestandteil zu zerlegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT bzw. die Bibliotheken können nicht so elegant wie `scikit-learn` mit `DataFrame`s umgehen, daher musst du die Daten in Arrays konvertieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sdf[\"title\"].values\n",
    "labels = sdf[\"success\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vielen Layern benötigt das BERT-Modell Platz, der proportional zur Maximallänge der Text ist. Um hier zu sparen, bestimmst du zunächst maximale Länge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(tokenizer.encode(t, add_special_tokens=True)) for t in text])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bestimmst du die *Input IDs* und die *Attention Masks*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,    # '[CLS]' und '[SEP]'\n",
    "                        max_length = 64,\n",
    "                        truncation = True,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,  # Attention-Masks erzeugen\n",
    "                        return_tensors = 'pt',         # pytorch-Tensoren als Ergebnis\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus technischen Gründen musst du die oben erzeugten Listen jetzt in *Tensoren* wandeln, mit denen `PyTorch` als Basisobjekte arbeitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du dir anzeigen lassen, was der `BertTokenizer` aus deinem ersten Dokument gemacht hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0])\n",
    "print(tokenizer.tokenize(text[0]))\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, hat der Tokenizer auch Subworte erkannt. Nur diese Worte kennt das dazugehörige Modell auch, deshalb ist es absolut entscheidend, immer den passenden Tokenizer mit dem dazueghörigen Modell zu verwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erzeugst du ein Datenset, das - du ahnst es bereits - auch wieder im Tensor-Format vorliegen muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei `scikit-learn` gibt es auch hier eine Hilfsfunktion, mit der du das Datenset in Trainings- und Testdaten aufteilst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# reproduzierbar arbeiten!\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(train_size, val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anders als bei `scikit-learn` arbeitet das Training in sog. *Batches*, deren Größe du hier mit 32 festlegst, wie der BERT-Autoren das empfehlen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend erzeugst du sog. `DataLoader`, die dir die Daten für die Batches genau so bereitstellen, wie du sie jeweils brauchst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun instanziierst du das Modell, dort musst du den gleichen Namen wie beim `BertTokenizer` oben verwenden. Wenn du keine Grafikkarte hast, musst du dich auf sehr lange Wartezeiten einstellen und `model.cuda()` durch `model.cpu()` ersetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# das Modell muss zum Tokenizer passen!\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2, # wir haben nur gut oder shlecht\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False # wir benötigen keine Embeddings\n",
    ")\n",
    "# hier evtl. model.cpu() einsetzen\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell versucht anschließend, den sog *Loss* (also die fehlerhaft klassifizierten Daten) zu minimieren. Dazu gibt es verschiedene Strategien, `AdamW` ist eigentlich die gebräuchliste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierer auswählen, AdamW ist Standard\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell wird in sog. `Epochen` trainiert, also einfach mehrmals hintereinander. Für unser Beispiel wählst du vier Epochen und erzeugst zusammen mit den Batches und der Größe des Trainingsets einen darauf angepassten *Scheduler*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# vier Epochen, das muss evtl. justiert werden\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du benötigst nun noch eine Funktion, die dir die Accuracy berechnet (durch das ausgeglichene Datenset ist das hier völlig in Ordnung). Die ist ein bisschen komplizierter. Von den Vorhersagen nutzt du nur die mit der jeweils größten Wahrscheinlichkeit für das Label. Da die Accuracy immer für einen ganzen *Batch* ausgerechnet werden muss, musst du die richtigen Vorhersagen zusammenzählen und durch die Anzahl der Datensätze teilen (`len(labels_flat)`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kannst du mit den eigentlichen Training beginnnen. Da die Funktion relativ kompliziert ist, werden wir sie im nächsten Teil genauer besprechen.\n",
    "\n",
    "Bitte brich hier das Notebook nicht ab, sondern fahre in der nächsten Lektion genau an dieser stelle fort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wunderbar! Genau hier!\n",
    "\n",
    "Wir schauen uns jetzt gemeinsam die Funktion zum Training an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "# alle Zufallszahlengeneratoren initialisieren (Reproduzierbarkeit)\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Statistik für das Training\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in trange(epochs, desc=\"Epoche\"):\n",
    "    # akkumulierter Loss für diese Epoche\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Modell in Trainingsmodus stellen\n",
    "    model.train()\n",
    "\n",
    "    # Batchweise trainieren\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # Daten entpacken und in device-Format wandeln\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Gradienten löschen\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Vorwärts-Auswertung (Trainingsdaten vorhersagen)\n",
    "        res = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Loss berechnen und akkumulieren\n",
    "        total_train_loss += res.loss.item()\n",
    "\n",
    "        # Rückwärts-Auswertung, um Gradienten zu bestimmen\n",
    "        res.loss.backward()\n",
    "\n",
    "        # Gradient beschränken wegen Exploding Gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Parameter und Lernrate aktualisieren\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Mittleren Loss über alle Batches berechnen\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "      \n",
    "\n",
    "    # Modell in Vorhersage-Modus umstellen\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validierung\"):\n",
    "        # jetzt die Validierungs-Daten entpacken\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Rückwärts-Auswertung wird nicht benötigt, daher auch kein Gradient\n",
    "        with torch.no_grad():        \n",
    "            # Vorhersage durchführen\n",
    "            res = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Loss akkumulieren\n",
    "        total_eval_loss += res.loss.item()\n",
    "\n",
    "        # Vorhersagedaten in CPU-Format wandeln, um Accuracy berechnen zu können\n",
    "        logits = res.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Gesamt-Accuraccy für diese Validierung ausgeben.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    tqdm.write(\"Accuracy: %f\" % avg_val_accuracy)\n",
    "\n",
    "    # Gesamte Loss über alle batches berechnen\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    tqdm.write(\"Validation loss %f\" % avg_val_loss)\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validierung Loss': avg_val_loss,\n",
    "            'Accuracy': avg_val_accuracy\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse (d.h. den Loss und die Accuracy) kannst du am besten in einem `DataFrame` darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lass dir das auch visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte sich vorstellen, dass für das Training auch drei Epochen genügt hätten. BERT-Modelle haben leider auch eine gewisse Tendenz zum *Overfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning als Basis für eigene Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT ist besonders leistungsfähig, weil du es auf deine eigenen Bedürfnisse anpassen kannst. Dazu benötigst du das *Finetuning*, das du gerade kennengelernt hast.\n",
    "\n",
    "In diesem Szenario hat das Training nicht sehr lange gedauert. Je nach Datenmenge und auch abhängig von deiner Hardware-Ausstattung kann das bei dir ganz anders aussehen. Im Vergleich zu den einfachen Machine-Learning-Modellen brauchen diese Deep-Learning-Methoden jedenfalls fast immer sehr viel mehr Rechenzeit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
