{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaskadiertes Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du im letzten Teil gesehen hast, funktioniert das automatische Beantworten von Fragen schon sehr gut, wenn sich die entsprechende Antwort im Dokument befindet.\n",
    "\n",
    "Leider hast du es oft nicht nur mit einem einzelnen Dokument zu tun, sondern mit sehr vielen. Da sieht die Sache etwas schwieriger aus, denn du musst erst das richtige Dokument finden, das potenziell die Antwort auf deine Frage enthält.\n",
    "\n",
    "Dazu benötigst du ein *kaskadiertes Modell*, das dir aus einer großen Menge von Dokumenten erst das richtige heraussucht. Mit den Techniken, die du bisher gelernt hast, könntest du das selbst herausfinden, z.B. mithilfe der semantischen Transformation. Es gibt allerdings auch schon fertig Frameworks, die das für dich erledigen. Ein solches wirst du dir jetzt genauer anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie beim Transfer Learning gewohnt lädst du die Original-Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT id, datePublished, url, full_text FROM nlp_articles WHERE datePublished<'2021-01-01' \", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzung des Haystack-Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dir Programmieraufwand für die Selektion der richtigen Daten zu sparen, kannst du das [Haystack-Framework](https://haystack.deepset.ai) verwenden. Es ist relativ umfangreich, du benötigst allerdings nur die Teile, die dir die Dokumente heraussuchen und dann Fragen dazu beantworten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/deepset-ai/haystack.git\n",
    "!pip install urllib3==1.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preprocessor.cleaning import clean_wiki_text\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.reader.transformers import TransformersReader\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Haystack* unterstützt unterschiedliche Backends für die Suche nach den richtigen Dokumente. ElasticSearch ist für unsere Anforderungen zu komplex, daher nutzt du nur den internen *Memory Store*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Dokumente müssen dafür ein eine Form konvertiert werden, in der neben dem reinen Text auch noch Metainformationen gespeichert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [{\"content\": row[\"full_text\"], \"meta\": {\"name\": row[\"url\"]}} for i, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend kannst du die Dokumente abspeichern, dabei passiert allerdings noch ziemlich wenig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erst wenn du das erste Mal auf die Daten zugreifst, werden diese gewandelt. Sehr praktisch daran ist, dass einzelne Paragraphen separat behandelt werden. Oft ist der Zusammenhang ja auch dadurch gegeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import TfidfRetriever\n",
    "retriever = TfidfRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel verwendest du entweder das von Deepset empfohlene Modell oder das aus der vorherigen Lektion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "reader = FARMReader(model_name_or_path=\"Sahajtomar/German-question-answer-Electra\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine *Pipeline* besteht hier aus einem Reader und einem Retriever. Es werden also zuerst die passenden Dokumente gefunden und anschließend wird versucht, mithilfe dieser Dokumente die Frage zu beantworten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du Fragen an den gesamten Heise Newsticker des Jahres 2020 stellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(query=\"Wer ist der Chef von Apple?\") \n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das hat ganz gut geklappt, versuche es nun mit einer etwas spezielleren Frage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(query=\"Wie viel verdient Tim Cook?\")\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider nicht ganz die passende Antwort. Wechsle daher den Konzern und stelle eine Frage zu Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(query=\"Wie hoch ist der Gewinn von Google?\")\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Richtung stimmt, aber das passt auch nicht ganz.\n",
    "\n",
    "Vielleicht klappt es besser mit allgemeinen Begriffen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(query=\"Was ist Corona?\")\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist schon ziemlich gut!\n",
    "\n",
    "Starte noch einen Versuch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(query=\"Was ist das wichtigste Produkt von Microsoft?\")\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte fast meinen, das System merkt sich die Fragen. *Teams* ist vielleicht aktuell wirklich für viele Menschen das wichtigste Produkt von Microsoft!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragen aus vielen Dokumenten zu beantworten ist schwierig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht umsonst haben sich Chatbots noch nicht so richtig durchgesetzt. Wie du siehst, ist es nicht so einfach, aus einer großen Dokumentemenge das richtige auszuwählen und Fragen korrekt zu beantworten.\n",
    "\n",
    "Das liegt daran, dass sich hier zwei schwierige Probleme miteinander verbinden:\n",
    "* Information Retrieval\n",
    "* Question Answering\n",
    "\n",
    "In beiden Bereichen gibt es aktuell große Fortschritte. In vielleicht sogar kurzer Zeit kannst du solche Anforderungen daher möglicherweise bereits viel besser lösen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
