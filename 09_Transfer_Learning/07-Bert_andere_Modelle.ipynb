{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit anderen Modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast jetzt schon ein BERT-Finetuning durchgeführt und das Ergebnis mit einer \"einfachen\" SVM-Klassifikation verglichen.\n",
    "\n",
    "Im Gegensatz zu SVM stellt dir BERT allerdings ein ganzes Ökosystem von Modellen, Optimierungen etc. zur Verfügung. Um das effektiv nutzen und bewerten zu können, musst du das BERT-Notebook nur ein kleines bisschen modifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben BERT selbst enthalten auch alle anderen Transfer-Learning-Modelle einen Tokenisierer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT id, datePublished, title, commentCount FROM articles \\\n",
    "                    WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten für Klassifikation vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klassifikationsaufgabe lässt du unverändert, um die Ergebnisse besser vergleichen zu können. \n",
    "\n",
    "Zunächst normalisierst du wie gewohnt die Kommentare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalizedCommentCount\"] = df[\"commentCount\"].fillna(0).map(int)\n",
    "df.loc[df[\"normalizedCommentCount\"]>500, \"normalizedCommentCount\"] = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann konstruierst du zwei `DataFrame`, in denen erfolgreich und nicht erfolgreiche Posts enthalten sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"normalizedCommentCount\"]>50].copy()\n",
    "df_success[\"success\"] = 1\n",
    "\n",
    "df_no_success = df[df[\"normalizedCommentCount\"]<10].copy()\n",
    "df_no_success[\"success\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du berechnest die Größe des kleineren `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_success = min(len(df_success), len(df_no_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und erzeugst ein ausgeglichenes Trainingsset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.concat([df_success.sample(min_success, random_state=42),\n",
    "                 df_no_success.sample(min_success, random_state=42)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für diesen Teil musst du sehr umfangreiche Berchnungen durchführen. Diese funktionieren zwar grundsätzlich auch auf einer CPU, allerdings würde das viele Stunden dauern. \n",
    "\n",
    "Wenn möglichst, solltest du den Code daher auf einer Grafikkarte laufen lassen, auf der das viele Größenordnungen schneller funktioniert. Solltest du auf keine Grafikkarte zugreifen können, lohnt es sich, dieses Noteboook in Google Colab auszuführen und eine entsprechende Umgebung auszuwählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU %s\" % torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU :-(\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `GeForce RTX 2070` ist zwar nicht ganz modern, aber für unsere Zwecke ist sie absolut ausreichend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der `transformers`-Bibliothen nutzt du zunächst den `AutoTokenizer`. Je nachdem, welches Modell du angibst, werden dann die  entsprechenden Klassen geladen und instanziiert. Das `dbmdz/bert-base-german-uncased` wurde von der Bayerischen Staatsbibliothek trainiert und ist wieder ein BERT-Modell. Alternativ kannst du `roberta-base` einsetzen, dann wird mit *RobertA* trainiert.\n",
    "\n",
    "Eine Übersicht über weitere Modelle findest du unter https://huggingface.co/models. Dort stehen dir umfangreiche Such- und Filtermöglichkeiten zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'dbmdz/bert-base-german-uncased'\n",
    "#model_name = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier musst du die Daten in Arrays konvertieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sdf[\"title\"].values\n",
    "labels = sdf[\"success\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vielen Layern benötigt das Modell Platz, der proportional zur Maximallänge der Text ist. Um hier zu sparen, bestimmst du zunächst maximale Länge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(tokenizer.encode(t, add_special_tokens=True)) for t in text])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bestimmst du die *Input IDs* und die *Attention Masks*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,    # '[CLS]' und '[SEP]'\n",
    "                        max_length = 64,\n",
    "                        truncation = True,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,  # Attention-Masks erzeugen\n",
    "                        return_tensors = 'pt',         # pytorch-Tensoren als Ergebnis\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus technischen Gründen musst du die oben erzeugten Listen jetzt in *Tensoren* wandeln, mit denen `PyTorch` als Basisobjekte arbeitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du dir anzeigen lassen, was der `AutoTokenizer` aus deinem ersten Dokument gemacht hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0])\n",
    "print(tokenizer.tokenize(text[0]))\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, hat der Tokenizer andere Subworte erkannt. Das sieht nun sehr viel mehr nach deutscher Sprache aus, was gut ist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erzeugst du ein Datenset, das - du ahnst es bereits - auch wieder im Tensor-Format vorliegen muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei `scikit-learn` gibt es auch hier eine Hilfsfunktion, mit der du das Datenset in Trainings- und Testdaten aufteilst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# reproduzierbar arbeiten!\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(train_size, val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anders als bei `scikit-learn` arbeitet das Training in sog. *Batches*, deren Größe du hier mit 32 festlegst, wie der BERT-Autoren das empfehlen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend erzeugst du sog. `DataLoader`, die dir die Daten für die Batches genau so bereitstellen, wie du sie jeweils brauchst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun instanziierst du das Modell, dort musst du den gleichen Namen wie beim `AutoTokenizer` oben verwenden. Wenn du keine Grafikkarte hast, musst du dich auf sehr lange Wartezeiten einstellen und `model.cuda()` durch `model.cpu()` ersetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# das Modell muss zum Tokenizer passen!\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels = 2, # wir haben nur gut oder shlecht\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False # wir benötigen keine Embeddings\n",
    ")\n",
    "# hier evtl. model.cpu() einsetzen\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell versucht anschließend, den sog *Loss* (also die fehlerhaft klassifizierten Daten) zu minimieren. Dazu gibt es verschiedene Strategien, `AdamW` ist eigentlich die gebräuchliste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierer auswählen, AdamW ist Standard\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell wird in sog. *Epochen* trainiert, also einfach mehrmals hintereinander. Für unser Beispiel wählst du vier Epochen und erzeugst zusammen mit den Batches und der Größe des Trainingsets einen darauf angepassten *Scheduler*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# vier Epochen, das muss evtl. justiert werden\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du benötigst nun noch eine Funktion, die dir die Accuracy berechnet (durch das ausgeglichene Datenset ist das hier völlig in Ordnung). Die ist ein bisschen komplizierter. Von den Vorhersagen nutzt du nur die mit der jeweils größten Wahrscheinlichkeit für das Label. Da die Accuracy immer für einen ganzen *Batch* ausgerechnet werden muss, musst du die richtigen Vorhersagen zusammenzählen und durch die Anzahl der Datensätze teilen (`len(labels_flat)`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt kannst du mit den eigentlichen Training beginnnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "# alle Zufallszahlengeneratoren initialisieren (Reproduzierbarkeit)\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Statistik für das Training\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in trange(epochs, desc=\"Epoche\"):\n",
    "    # akkumulierter Loss für diese Epoche\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Modell in Trainingsmodus stellen\n",
    "    model.train()\n",
    "\n",
    "    # Batchweise trainieren\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # Daten entpacken und in device-Format wandeln\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Gradienten löschen\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Vorwärts-Auswertung (Trainingsdaten vorhersagen)\n",
    "        res = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Loss berechnen und akkumulieren\n",
    "        total_train_loss += res.loss.item()\n",
    "\n",
    "        # Rückwärts-Auswertung, um Gradienten zu bestimmen\n",
    "        res.loss.backward()\n",
    "\n",
    "        # Gradient beschränken wegen Exploding Gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Parameter und Lernrate aktualisieren\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Mittleren Loss über alle Batches berechnen\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "      \n",
    "\n",
    "    # Modell in Vorhersage-Modus umstellen\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validierung\"):\n",
    "        # jetzt die Validierungs-Daten entpacken\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Rückwärts-Auswertung wird nicht benötigt, daher auch kein Gradient\n",
    "        with torch.no_grad():        \n",
    "            # Vorhersage durchführen\n",
    "            res = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Loss akkumulieren\n",
    "        total_eval_loss += res.loss.item()\n",
    "\n",
    "        # Vorhersagedaten in CPU-Format wandeln, um Accuracy berechnen zu können\n",
    "        logits = res.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Gesamt-Accuraccy für diese Validierung ausgeben.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    tqdm.write(\"Accuracy: %f\" % avg_val_accuracy)\n",
    "\n",
    "    # Gesamte Loss über alle batches berechnen\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    tqdm.write(\"Validation loss %f\" % avg_val_loss)\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validierung Loss': avg_val_loss,\n",
    "            'Accuracy': avg_val_accuracy\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse sind deutlich besser als bei dem bisherigen Modell!\n",
    "\n",
    "Den Loss und die Accuracy stellst du am besten in einem `DataFrame` dar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lass dir das auch visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell konvergiert auch deutlich schneller. Vermutlich liegt das daran, dass die deutschen Sprachkonstrukte viel tiefer eingebettet sind. Evtl. würden auch weniger Trainingsdaten genügen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnis ist modellabhängig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie du siehst, hängt das Ergebnis des Finetunings nicht unerheblich vom verwendeten Modell ab. Hier gibt es sehr viele Variationsmöglichkeiten, so kannst du z.B. auch *RobertA*, *DistilBERT* oder *XLNet* verwenden. Sehr viele Sprachmodelle findest du dazu bei [Huggingface](https://huggingface.co/models). Dort gibt es auch noch speziellere Modelle, die z.B. auf Sentiments trainiert sind oder Übersetzungen vornehmen können. Der Fantasie sind da keine Grenzen gesetzt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
